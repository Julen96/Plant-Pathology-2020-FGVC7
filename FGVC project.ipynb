{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.resnet import ResNet50\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(train, directory):\n",
    "    \n",
    "    # This function loads the images, resizes them and puts them into an array\n",
    "    \n",
    "    img_size = 224\n",
    "    train_image = []\n",
    "    for name in train['image_id']:\n",
    "        path = directory + 'images/' + name + '.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        train_image.append(img)\n",
    "    train_image_array = np.array(train_image)\n",
    "    \n",
    "    return train_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_augmented(directory):\n",
    "    \n",
    "    # This function loads the augmented images and the augmented csv file\n",
    "    df_train = pd.read_csv(directory + 'augmented.csv')\n",
    "    \n",
    "    train_image = []\n",
    "    for name in df_train['image_id']:\n",
    "        path = directory + 'images_resized_augmented/' + name + '.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        train_image.append(img)\n",
    "    train_image_array = np.array(train_image)\n",
    "    \n",
    "    return  train_image_array, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/julen/OneDrive/Escritorio/IA/CS577-Deep-Learning/Project/'\n",
    "x_train, df_train = load_images_augmented(directory)\n",
    "df_test = pd.read_csv(directory + 'test.csv')\n",
    "x_test = load_images(df_test, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize -> We don't because of memory issues. A float32 occupies 4x more than the original uint8\n",
    "# x_train = x_train / 255.0 \n",
    "# x_test =  x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x_train:  (7284, 224, 224, 3)\n",
      "Size of x_val:  (1821, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = df_train[['healthy', 'multiple_diseases', 'rust', 'scab']].to_numpy()\n",
    "\n",
    "x_train_original, y_train_original = shuffle(x_train, y_train)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_original, y_train_original, test_size = 0.2, random_state = 2020)\n",
    "\n",
    "print('Size of x_train: ', x_train.shape)\n",
    "print('Size of x_val: ', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, x_test, name):\n",
    "    x_pred = model.predict(x_test)\n",
    "    df_test['healthy'] = x_pred[:,0]\n",
    "    df_test['multiple_diseases'] = x_pred[:,1]\n",
    "    df_test['rust'] = x_pred[:,2]\n",
    "    df_test['scab'] = x_pred[:,3]\n",
    "    df_test.to_csv(name, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2070,  346, 2495, 2373], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See distribution of samples\n",
    "y_train.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach: Batch Confusion Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plot(history):\n",
    "\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(model, x_test, y_test):\n",
    "    score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASSP(output, act_name): # TODO: Check if it is well built. Not like in paper. Maybe different image size?\n",
    "    ASSP = Conv2D(filters = 2048, kernel_size = (1,1))(output)\n",
    "    ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 2)(ASSP)\n",
    "#     ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 4)(ASSP)\n",
    "    # ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 6)(ASSP)\n",
    "    # ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 7)(ASSP)\n",
    "\n",
    "    ASSP = Activation(act_name)(ASSP)\n",
    "    \n",
    "    return ASSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_bcn_loss(y_pred):\n",
    "\n",
    "    \n",
    "    matrix = K.dot(K.transpose(y_pred), y_pred)\n",
    "    eigenvalues, eigenvectors = tf.linalg.eigh(matrix)\n",
    "    bcn_loss = K.sum(eigenvalues) \n",
    "    \n",
    "    print(eigenvalues)\n",
    "    print(type(eigenvalues))\n",
    "    print(type(y_pred))\n",
    "    print('y_pred shape: ', y_pred.shape)\n",
    "    print('yPred.transpose shape: ', K.transpose(y_pred).shape)\n",
    "    print('matrix shape: ', matrix.shape)\n",
    "    print('eigenvalues shape: ', eigenvalues.shape)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return bcn_loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    categorical_crossentropy = losses.categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    BCN_loss = compute_bcn_loss(y_pred)\n",
    "    \n",
    "    loss_total = categorical_crossentropy + BCN_loss\n",
    "    \n",
    "    return loss_total\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss/dense_2_loss/custom_loss/SelfAdjointEigV2:0\", shape=(4,), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "y_pred shape:  (None, 4)\n",
      "yPred.transpose shape:  (4, None)\n",
      "matrix shape:  (4, 4)\n",
      "eigenvalues shape:  (4,)\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Model)                (None, 7, 7, 2048)   23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 2048)   4196352     resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 2048)   4196352     resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 3, 3, 2048)   37750784    conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 2048)   37750784    conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 3, 3, 2048)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 3, 3, 2048)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 3, 3, 2048)   0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18432)        0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          4718848     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            1028        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 112,201,860\n",
      "Trainable params: 112,148,740\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (224, 224, 3))\n",
    "out = model_resnet(inputs)\n",
    "features = ASSP(out, 'relu')\n",
    "attention = ASSP(out, 'sigmoid')\n",
    "tensor_total = Multiply()([features, attention])\n",
    "tensor_total_flat = Flatten()(tensor_total)\n",
    "tensor_total_flat = Dense(256, activation = 'relu')(tensor_total_flat) # TODO: Check if it should be relu or another\n",
    "tensor_total_flat = Dense(4, activation = 'softmax')(tensor_total_flat)\n",
    "\n",
    "model = Model(inputs=inputs, outputs= tensor_total_flat)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = custom_loss, metrics = ['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7284 samples, validate on 1821 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[3,3,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node mul_1092 (defined at C:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_50206]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-097070a4f8de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[3,3,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node mul_1092 (defined at C:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_50206]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 2, epochs = 3, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In order to set the batch size and each batch with different labels, we should use fit_generator or train_on_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, batch_size, epochs, verbose = True):\n",
    "    \n",
    "    \n",
    "    ##model building\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu', input_shape = (224, 224, 3)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "\n",
    "    print(model.summary())\n",
    "    start_time = time.time()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['categorical_accuracy'])\n",
    "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), batch_size = batch_size, epochs = epochs, verbose = verbose)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return (model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 220, 220, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 108, 108, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 54, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 52, 52, 512)       590336    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 1024)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 6, 6, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               4718720   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,122,436\n",
      "Trainable params: 10,122,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7284 samples, validate on 1821 samples\n",
      "Epoch 1/1\n",
      "7284/7284 [==============================] - 115s 16ms/step - loss: 2.8426 - categorical_accuracy: 0.3471 - val_loss: 1.2473 - val_categorical_accuracy: 0.3773\n",
      "--- 116.12400031089783 seconds ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-67daff237a33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5ccc60850d14>\u001b[0m in \u001b[0;36mmodel_plot\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "model_old, history = build_model(x_train, y_train, batch_size = 16, epochs = 1)\n",
    "model_plot(history)\n",
    "print_score(model_old, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['healthy'] = x_pred[:,0]\n",
    "df_test['multiple_diseases'] = x_pred[:,1]\n",
    "df_test['rust'] = x_pred[:,2]\n",
    "df_test['scab'] = x_pred[:,3]\n",
    "df_test.to_csv('FGVC_submission_.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Test_0</td>\n",
       "      <td>0.205251</td>\n",
       "      <td>0.045785</td>\n",
       "      <td>0.368202</td>\n",
       "      <td>0.380761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Test_1</td>\n",
       "      <td>0.231453</td>\n",
       "      <td>0.047746</td>\n",
       "      <td>0.345878</td>\n",
       "      <td>0.374923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Test_2</td>\n",
       "      <td>0.223623</td>\n",
       "      <td>0.071676</td>\n",
       "      <td>0.342834</td>\n",
       "      <td>0.361867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Test_3</td>\n",
       "      <td>0.308657</td>\n",
       "      <td>0.102331</td>\n",
       "      <td>0.295816</td>\n",
       "      <td>0.293197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Test_4</td>\n",
       "      <td>0.230924</td>\n",
       "      <td>0.044711</td>\n",
       "      <td>0.364794</td>\n",
       "      <td>0.359571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>Test_1816</td>\n",
       "      <td>0.220903</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>0.343984</td>\n",
       "      <td>0.381910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1817</td>\n",
       "      <td>Test_1817</td>\n",
       "      <td>0.273419</td>\n",
       "      <td>0.108838</td>\n",
       "      <td>0.302602</td>\n",
       "      <td>0.315141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>Test_1818</td>\n",
       "      <td>0.324635</td>\n",
       "      <td>0.099112</td>\n",
       "      <td>0.290823</td>\n",
       "      <td>0.285430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1819</td>\n",
       "      <td>Test_1819</td>\n",
       "      <td>0.337709</td>\n",
       "      <td>0.099663</td>\n",
       "      <td>0.284925</td>\n",
       "      <td>0.277703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>Test_1820</td>\n",
       "      <td>0.201312</td>\n",
       "      <td>0.038694</td>\n",
       "      <td>0.375525</td>\n",
       "      <td>0.384469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows Ã— 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id   healthy  multiple_diseases      rust      scab\n",
       "0        Test_0  0.205251           0.045785  0.368202  0.380761\n",
       "1        Test_1  0.231453           0.047746  0.345878  0.374923\n",
       "2        Test_2  0.223623           0.071676  0.342834  0.361867\n",
       "3        Test_3  0.308657           0.102331  0.295816  0.293197\n",
       "4        Test_4  0.230924           0.044711  0.364794  0.359571\n",
       "...         ...       ...                ...       ...       ...\n",
       "1816  Test_1816  0.220903           0.053204  0.343984  0.381910\n",
       "1817  Test_1817  0.273419           0.108838  0.302602  0.315141\n",
       "1818  Test_1818  0.324635           0.099112  0.290823  0.285430\n",
       "1819  Test_1819  0.337709           0.099663  0.284925  0.277703\n",
       "1820  Test_1820  0.201312           0.038694  0.375525  0.384469\n",
       "\n",
       "[1821 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach: Localization, Pooling and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer, InputSpec\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GlobalKMaxPooling2D(Layer): #Inherits the properties of Layer class\n",
    "    \"\"\"K Max Pooling operation for spatial data.\n",
    "    \n",
    "    # Arguments\n",
    "        \n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` (default) or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `\"channels_first\"`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be `\"channels_last\"`.\n",
    "            \n",
    "        K: An Integer,\n",
    "            states the number of selected maximal values over which the\n",
    "            average is going to be computed.\n",
    "            \n",
    "            \n",
    "    # Input shape\n",
    "    \n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, rows, cols, channels)`\n",
    "            \n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, rows, cols)`\n",
    "            \n",
    "    # Output shape\n",
    "    \n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, pooled_rows, pooled_cols, channels)`\n",
    "            \n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, pooled_rows, pooled_cols)`    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, data_format=None, k = 10, **kwargs):\n",
    "        super(GlobalKMaxPooling2D, self).__init__(**kwargs)\n",
    "        self.data_format = K.normalize_data_format(data_format)\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'data_format': self.data_format, 'k' : self.k}\n",
    "        base_config = super(GlobalKMaxPooling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if self.data_format == 'channels_last':\n",
    "            # Here first sort\n",
    "            # Then take K maximum values\n",
    "            # Then average them\n",
    "            k = self.k\n",
    "\n",
    "            input_reshaped = tf.reshape(inputs, [tf.shape(inputs)[0], -1, tf.shape(inputs)[3]])\n",
    "            input_reshaped = tf.reshape(input_reshaped, [tf.shape(input_reshaped)[0], tf.shape(input_reshaped)[2], tf.shape(input_reshaped)[1]])\n",
    "            top_k = tf.math.top_k(input_reshaped, k=k, sorted = True, name = None)[0]\n",
    "            mean = tf.keras.backend.mean(top_k, axis = 2)\n",
    "            #assert ((input_reshaped.get_shape()[0], input_reshaped.get_shape()[-1]) == mean.get_shape())\n",
    "        \n",
    "        return mean\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n",
    "               lr_min=0.00001, lr_rampup_epochs=5, \n",
    "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "    lr_max = lr_max #* strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) *\\\n",
    "                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n",
    "                                - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    return lrfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfn = build_lrfn()\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmbeddingLayerLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b273137d100b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# model.add(Dense(128, activation = 'relu'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbeddingLayerLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EmbeddingLayerLoss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(model_resnet)\n",
    "model.add(GlobalKMaxPooling2D(data_format = 'channels_last' , k = 4))\n",
    "# model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = EmbeddingLayerLoss(model.layers[2]), metrics = ['categorical_accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet import ResNet50\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "print(tf.executing_eagerly())\n",
    "    \n",
    "class ElopeModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Model\n",
    "        self.model = self.build_model()\n",
    "        # Print a model summary\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = 'Adam'\n",
    "        \n",
    "        # Have to convert to tensor??\n",
    "        self.loss_parameters = {'means' : 0, 'lr' : 0.5, 'landa' : 2.0,\n",
    "                               'gamma' : 16.0, 'm' : 0.75}\n",
    "\n",
    "        \n",
    "        #Loss Function\n",
    "        self.loss_func = self.model_loss()\n",
    "        \n",
    "        self.compile()\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(model_resnet)\n",
    "        model.add(GlobalKMaxPooling2D(data_format = 'channels_last' , k = 4))\n",
    "        model.add(Dense(4, activation = 'softmax'))\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def calculate_within_class_loss(self, y_true, y_pred, fx_tensor): #fx_tensor = Output of layer\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        # Layer there or in self??\n",
    "        mean_tensor = self.loss_parameters['means']\n",
    "        alpha = self.loss_parameters['lr']\n",
    "\n",
    "        mean_new = [[], [], [], []] # 4 classes\n",
    "\n",
    "        dimension = K.shape(fx_tensor)[1]\n",
    "        num_classes = K.shape(y_true)[1]\n",
    "        if isinstance(mean_tensor, int): # Initialize them if they are not yet initialized\n",
    "            mean_tensor = tf.zeros([num_classes, dimension], dtype = tf.dtypes.float64)\n",
    "#         num_samples_in_batch = tf.shape(fx_tensor)[0].numpy()\n",
    "        num_samples_in_batch = K.shape(fx_tensor)[0]\n",
    "        \n",
    "        # Till now everything great. But as we have to do a loop, we need number of classes and number of samples\n",
    "        # Per batch to be integers, not tensors\n",
    "        \n",
    "        num_samples_in_batch = 4\n",
    "        num_classes = 4\n",
    "\n",
    "        # LOOP THROUGH CLASSES TO CALCULATE NEW MEAN\n",
    "        for i in range(0, num_classes): # Loops throgh the classes\n",
    "            counter = 1 # to divide it\n",
    "            summ = tf.zeros(dimension, dtype = tf.dtypes.float64) # Initialize summ for delta \n",
    "            mean_slice = tf.slice(mean_tensor, [i, 0], [1,dimension])\n",
    "            for j in range(0, num_samples_in_batch): # Loops through the images in the batch  \n",
    "                y_true_slice = tf.slice(y_true, [j,0], [1,4]) # Class of the i image\n",
    "                class_number = tf.argmax(y_true_slice, output_type=tf.dtypes.int64) # Class goes from 0 to 3, in order to fit better into slice method.\n",
    "                                                                # Check if this is possible in the GPU\n",
    "                if class_number == tf.constant(i, dtype = tf.dtypes.int64): # if not, do nothing\n",
    "                    counter += 1\n",
    "                    fx_tensor_slice = tf.slice(fx_tensor, [j,0], [1, dimension]) #Begin, size\n",
    "                    summ = tf.add(summ, tf.subtract(mean_slice, fx_tensor_slice))\n",
    "                    \n",
    "            total = tf.divide(summ, tf.constant(counter, dtype = tf.dtypes.float64))\n",
    "#             assert(np.sum(total.numpy()) == np.sum(summ.numpy() / counter))\n",
    "            mean_new[i] = tf.subtract(mean_slice, tf.scalar_mul(tf.constant(alpha, dtype = tf.dtypes.float64), total))\n",
    "#             assert(np.sum(mean_new[i].numpy()) == np.sum(mean_slice.numpy() - alpha * total.numpy()))\n",
    "\n",
    "        # Assembly the new mean tensor\n",
    "        mean_tensor = tf.concat([mean_new[0], mean_new[1], mean_new[2], mean_new[3]], axis = 0)\n",
    "\n",
    "        # CALCULATE LOSS\n",
    "        loss = tf.Variable(0, dtype = tf.dtypes.float64)\n",
    "        for i in range (0, num_samples_in_batch):\n",
    "            y_true_slice = tf.slice(y_true, [i,0], [1,4]) # Take the label of the ith sample\n",
    "            class_number = np.argmax(y_true_slice) # Convert it to integer\n",
    "            mean_slice = tf.slice(mean_tensor, [class_number, 0], [1, dimension])\n",
    "            fx_tensor_slice = tf.slice(fx_tensor, [j,0], [1, dimension])\n",
    "            # Now do the subtract, square, sum and divide\n",
    "#             assert(np.sum(tf.shape(mean_slice).numpy()) == np.sum(tf.shape(fx_tensor_slice).numpy()))\n",
    "            rest = tf.subtract(fx_tensor_slice, class_slice)\n",
    "            norm = tf.square(tf.norm(rest))\n",
    "            loss = tf.add(loss, norm)\n",
    "\n",
    "        loss = tf.divide(loss, tf.Variable(2*num_samples_in_batch, dtype = tf.dtypes.float64))\n",
    "        \n",
    "        \n",
    "        \n",
    "        # N = number of samples in the batch\n",
    "        # f(xn) -> Dimension = 2048 (not the batch because it it xn, only 1 sample)\n",
    "        # class means -> should be the same as f(xn) x number of classes\n",
    "        # Therefore ||f(xn) - U(cn)||^2 will be a number, because f(xn) is a vector, not a matrix\n",
    "\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_between_class_loss(self, y_true, y_pred, layer_output):\n",
    "        # Layer there or in self??\n",
    "        class_means = self.loss_parameters['means']\n",
    "        gamma = self.loss_parameters['gamma']\n",
    "        m = self.loss_parameters['m']\n",
    "        # m -> Margin\n",
    "        # P -> class-pairs in the current batch\n",
    "        # |P| -> Cardinality of P, number of elements in the set\n",
    "        \n",
    "#         bc_loss = \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"\" Wrapper function which calculates auxiliary values for the complete loss function.\n",
    "         Returns a *function* which calculates the complete loss given only the input and target output \"\"\"\n",
    "        # This part has to be developed\n",
    "        \n",
    "        #Within class loss\n",
    "        within_class_loss_func = self.calculate_within_class_loss\n",
    "        \n",
    "        # Between class loss\n",
    "        between_class_loss_func = self.calculate_between_class_loss\n",
    "        \n",
    "        lay_out = self.model.layers[-2].output\n",
    "        \n",
    "        \n",
    "        def ElopeLoss(y_true, y_pred):\n",
    "            \n",
    "            # Within Class loss has to be computed first, in order to get the new class means updated\n",
    "            \n",
    "            within_class_loss = within_class_loss_func(y_true, y_pred, lay_out)\n",
    "            \n",
    "            between_class_loss = between_class_loss_func(y_true, y_pred, lay_out)\n",
    "            \n",
    "            cat_cross_loss = categorical_crossentropy(y_true, y_pred)\n",
    "            \n",
    "            model_loss = cat_cross_loss + within_class_loss #+ between_class_loss\n",
    "            \n",
    "            return model_loss\n",
    "        \n",
    "        \n",
    "        return ElopeLoss\n",
    "    \n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\" Compiles the Keras model. Includes metrics to differentiate between the two main loss terms \"\"\"\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        self.model.compile(optimizer = self.optimizer, loss = self.loss_func,\n",
    "                          metrics = [categorical_crossentropy]) # Here put the two losses better\n",
    "        print('Model Compiled!')\n",
    "        \n",
    "    \n",
    "    def load_trained_weights(self, weights):\n",
    "        \"\"\" Loads weights of a pre-trained model. 'weights' is path to h5 model\\weights file\"\"\"\n",
    "        self.model.load_weights(weights)\n",
    "        print('Weights from {} loaded successfully'.format(weights))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n",
      "Eager execution: True\n",
      "Model: \"sequential_28\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_k_max_pooling2d_28 (G (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 4)                 8196      \n",
      "=================================================================\n",
      "Total params: 23,595,908\n",
      "Trainable params: 23,542,788\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Eager execution: True\n",
      "Eager execution: False\n"
     ]
    },
    {
     "ename": "OperatorNotAllowedInGraphError",
     "evalue": "using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-81-aff0bf90bc94>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mElope\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mElopeModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-80-e2e5c5d49f03>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     32\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mloss_func\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     33\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 34\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     35\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     36\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-e2e5c5d49f03>\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    175\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Eager execution:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    176\u001b[0m         self.model.compile(optimizer = self.optimizer, loss = self.loss_func,\n\u001b[1;32m--> 177\u001b[1;33m                           metrics = [categorical_crossentropy]) # Here put the two losses better\n\u001b[0m\u001b[0;32m    178\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model Compiled!'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    179\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py\u001b[0m in \u001b[0;36msymbolic_fn_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0m_SYMBOLIC_SCOPE\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mcompile\u001b[1;34m(self, optimizer, loss, metrics, loss_weights, sample_weight_mode, weighted_metrics, target_tensors, **kwargs)\u001b[0m\n\u001b[0;32m    227\u001b[0m         \u001b[1;31m#                   loss_weight_2 * output_2_loss_fn(...) +\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    228\u001b[0m         \u001b[1;31m#                   layer losses.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 229\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtotal_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_total_loss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    230\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    231\u001b[0m         \u001b[1;31m# Functions for train, test and predict will\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_prepare_total_loss\u001b[1;34m(self, masks)\u001b[0m\n\u001b[0;32m    690\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    691\u001b[0m                     output_loss = loss_fn(\n\u001b[1;32m--> 692\u001b[1;33m                         y_true, y_pred, sample_weight=sample_weight)\n\u001b[0m\u001b[0;32m    693\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    694\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m     69\u001b[0m         \u001b[0mscope_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'lambda'\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'<lambda>'\u001b[0m \u001b[1;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     70\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 71\u001b[1;33m             \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     72\u001b[0m             return losses_utils.compute_weighted_loss(\n\u001b[0;32m     73\u001b[0m                 losses, sample_weight, reduction=self.reduction)\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    130\u001b[0m             \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    131\u001b[0m         \"\"\"\n\u001b[1;32m--> 132\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    133\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    134\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-e2e5c5d49f03>\u001b[0m in \u001b[0;36mElopeLoss\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m    156\u001b[0m             \u001b[1;31m# Within Class loss has to be computed first, in order to get the new class means updated\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    157\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 158\u001b[1;33m             \u001b[0mwithin_class_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mwithin_class_loss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlay_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    159\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    160\u001b[0m             \u001b[0mbetween_class_loss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbetween_class_loss_func\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlay_out\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-80-e2e5c5d49f03>\u001b[0m in \u001b[0;36mcalculate_within_class_loss\u001b[1;34m(self, y_true, y_pred, fx_tensor)\u001b[0m\n\u001b[0;32m     82\u001b[0m                 \u001b[0mclass_number\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true_slice\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_type\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m# Class goes from 0 to 3, in order to fit better into slice method.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     83\u001b[0m                                                                 \u001b[1;31m# Check if this is possible in the GPU\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 84\u001b[1;33m                 \u001b[1;32mif\u001b[0m \u001b[0mclass_number\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconstant\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mint64\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m# if not, do nothing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     85\u001b[0m                     \u001b[0mcounter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     86\u001b[0m                     \u001b[0mfx_tensor_slice\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mslice\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfx_tensor\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mj\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdimension\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#Begin, size\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    755\u001b[0m       \u001b[0;31m`\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;31m`\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    756\u001b[0m     \"\"\"\n\u001b[1;32m--> 757\u001b[1;33m     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_disallow_bool_casting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    758\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    759\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m__nonzero__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_disallow_bool_casting\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    524\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    525\u001b[0m       \u001b[1;31m# Default: V1-style Graph execution.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 526\u001b[1;33m       \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_disallow_in_graph_mode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"using a `tf.Tensor` as a Python `bool`\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    527\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    528\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_disallow_iteration\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\framework\\ops.py\u001b[0m in \u001b[0;36m_disallow_in_graph_mode\u001b[1;34m(self, task)\u001b[0m\n\u001b[0;32m    513\u001b[0m     raise errors.OperatorNotAllowedInGraphError(\n\u001b[0;32m    514\u001b[0m         \u001b[1;34m\"{} is not allowed in Graph execution. Use Eager execution or decorate\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 515\u001b[1;33m         \" this function with @tf.function.\".format(task))\n\u001b[0m\u001b[0;32m    516\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    517\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_disallow_bool_casting\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mOperatorNotAllowedInGraphError\u001b[0m: using a `tf.Tensor` as a Python `bool` is not allowed in Graph execution. Use Eager execution or decorate this function with @tf.function."
     ]
    }
   ],
   "source": [
    "Elope = ElopeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'VERSION'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1b3b6cee90b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TensorFlow version: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVERSION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Eager execution: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'VERSION'"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7284 samples, validate on 1821 samples\n",
      "Epoch 1/1\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "7284/7284 [==============================] - 471s 65ms/step - loss: 1.6179 - categorical_crossentropy: 1.6179 - val_loss: 0.9083 - val_categorical_crossentropy: 0.9083\n"
     ]
    }
   ],
   "source": [
    "history = Elope.model.fit(x_train, y_train, batch_size = 12, epochs = 1, \n",
    "                    validation_data = (x_val, y_val), callbacks = [lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps needed for calculating the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to specify some random values and convert them to tensors\n",
    "\n",
    "f_x = np.random.rand(3,2048)\n",
    "mean = np.random.rand(4,2048)\n",
    "y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0]])\n",
    "\n",
    "fx_tensor = tf.convert_to_tensor(f_x)\n",
    "mean_tensor = tf.convert_to_tensor(mean)\n",
    "y_true_tensor = tf.convert_to_tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to get the necessary slices from the tensors\n",
    "\n",
    "fx_tensor_slice = tf.slice(f_x, [1,0], [1, tf.shape(fx_tensor)[1].numpy()]) #Begin, size\n",
    "y_true_slice = tf.slice(y_true, [1,0], [1,4])\n",
    "class_number = np.argmax(y_true_slice.numpy()) # Class goes from 0 to 3, in order to fit better into slice method\n",
    "mean_slice = tf.slice(mean_tensor, [class_number, 0], [1,tf.shape(mean_tensor)[1].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_x shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "mean shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "y_true shape:  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('f_x shape: ', tf.shape(fx_tensor_slice))\n",
    "print('mean shape: ', tf.shape(mean_slice) )\n",
    "print('y_true shape: ', tf.shape(y_true_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2048), dtype=float64, numpy=\n",
       "array([[-0.03519978, -0.10456176,  0.45651788, ..., -0.02443045,\n",
       "         0.12662611,  0.18093321]])>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_slice - fx_tensor_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.88533766, 0.67987946, 0.45612977, ..., 0.71605771, 0.17641876,\n",
       "        0.91872005],\n",
       "       [0.57770646, 0.38572611, 0.2277064 , ..., 0.22086407, 0.6774727 ,\n",
       "        0.10930888],\n",
       "       [0.20224636, 0.88990744, 0.64239133, ..., 0.12926163, 0.38676811,\n",
       "        0.10883774]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(fx_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n",
      "False\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Complete method\n",
    "\n",
    "# CANNOT USE NUMPY OR ARRAYS! Because it would make problems for backpropagation, as they are not in the graph.\n",
    "# Other way is to compute the backpropagation by ourselves.\n",
    "\n",
    "alpha = 0.5\n",
    "mean_new = [[], [], [], []] # 4 classes\n",
    "num_classes = tf.shape(mean_tensor)[0].numpy()\n",
    "num_samples_in_batch = tf.shape(fx_tensor)[0].numpy()\n",
    "dimension = tf.shape(mean_tensor)[1].numpy()\n",
    "\n",
    "# LOOP THROUGH CLASSES TO CALCULATE NEW MEAN\n",
    "for i in range(0, num_classes): # Loops throgh the classes\n",
    "\n",
    "    counter = 1 # to divide it\n",
    "    summ = tf.zeros(dimension, dtype = tf.dtypes.float64) # Initialize class means \n",
    "    # TODO: Initialize class means outside this function, as a parameter of self\n",
    "    mean_slice = tf.slice(mean_tensor, [i, 0], [1,dimension])\n",
    "    print(type(mean_slice) == 'numpy')\n",
    "    print(isinstance(0, int))\n",
    "    for j in range(0, num_samples_in_batch): # Loops through the images in the batch  \n",
    "        y_true_slice = tf.slice(y_true, [j,0], [1,4]) # Class of the i image\n",
    "        class_number = np.argmax(y_true_slice.numpy()) # Class goes from 0 to 3, in order to fit better into slice method.\n",
    "                                                        # Check if this is possible in the GPU\n",
    "\n",
    "        if class_number == i: # if not, do nothing\n",
    "            counter += 1\n",
    "            fx_tensor_slice = tf.slice(fx_tensor, [j,0], [1, dimension]) #Begin, size\n",
    "            summ = tf.add(summ, tf.subtract(mean_slice, fx_tensor_slice))\n",
    "        \n",
    "    total = tf.divide(summ, tf.constant(counter, dtype = tf.dtypes.float64))\n",
    "    assert(np.sum(total.numpy()) == np.sum(summ.numpy() / counter))\n",
    "    \n",
    "    mean_new[i] = tf.subtract(mean_slice, tf.scalar_mul(tf.constant(alpha, dtype = tf.dtypes.float64), total))\n",
    "    assert(np.sum(mean_new[i].numpy()) == np.sum(mean_slice.numpy() - alpha * total.numpy()))\n",
    "    \n",
    "# Assembly the new mean tensor\n",
    "mean_tensor = tf.concat([mean_new[0], mean_new[1], mean_new[2], mean_new[3]], axis = 0)\n",
    "    \n",
    "# CALCULATE LOSS\n",
    "loss = tf.Variable(0, dtype = tf.dtypes.float64)\n",
    "for i in range (0, num_samples_in_batch):\n",
    "    y_true_slice = tf.slice(y_true, [i,0], [1,4]) # Take the label of the ith sample\n",
    "    class_number = np.argmax(y_true_slice.numpy()) # Convert it to integer\n",
    "    mean_slice = tf.slice(mean_tensor, [class_number, 0], [1, dimension])\n",
    "    fx_tensor_slice = tf.slice(fx_tensor, [j,0], [1, dimension])\n",
    "    # Now do the subtract, square, sum and divide\n",
    "    assert(np.sum(tf.shape(mean_slice).numpy()) == np.sum(tf.shape(fx_tensor_slice).numpy()))\n",
    "    rest = tf.subtract(fx_tensor_slice, mean_slice)\n",
    "    norm = tf.square(tf.norm(rest))\n",
    "    loss = tf.add(loss, norm)\n",
    "    \n",
    "loss = tf.divide(loss, tf.Variable(2*num_samples_in_batch, dtype = tf.dtypes.float64))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2048), dtype=float64, numpy=\n",
       "array([[0.88533766, 0.67987946, 0.45612977, ..., 0.71605771, 0.17641876,\n",
       "        0.91872005],\n",
       "       [0.57770646, 0.38572611, 0.2277064 , ..., 0.22086407, 0.6774727 ,\n",
       "        0.10930888],\n",
       "       [0.20224636, 0.88990744, 0.64239133, ..., 0.12926163, 0.38676811,\n",
       "        0.10883774]])>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'op', 'value_index', and 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ad72b0175e28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'op', 'value_index', and 'dtype'"
     ]
    }
   ],
   "source": [
    "tf.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_tot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-faa0e8882ae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmean_tot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_tot' is not defined"
     ]
    }
   ],
   "source": [
    "mean_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_x shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "mean shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "y_true shape:  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('f_x shape: ', tf.shape(fx_tensor_slice))\n",
    "print('mean shape: ', tf.shape(mean_slice) )\n",
    "print('y_true shape: ', tf.shape(y_true_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor_broadcasted = tf.broadcast_to(mean_tensor, [tf.shape(f_x_tensor)[0].numpy(), tf.shape(mean_tensor)[1].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2048), dtype=float64, numpy=\n",
       "array([[ 0.52522555,  0.23903976,  0.00548459, ...,  0.69942552,\n",
       "         0.43518934,  0.03350187],\n",
       "       [ 0.18296376, -0.19688141, -0.83012728, ...,  0.15724875,\n",
       "        -0.33678901, -0.52118118],\n",
       "       [-0.10041236,  0.0561405 , -0.92195231, ...,  0.22434052,\n",
       "         0.45476284,  0.12300551]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.subtract(mean_tensor_broadcasted, f_x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tensor = tf.convert_to_tensor(y_true)\n",
    "y_true_tensor = tf.expand_dims(y_true_tensor, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 4, 1])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(y_true_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_slice.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_true_slice.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_save(model, x_test, name = 'FGVC_submission_2_approach.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_output = Elope.model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.convert_to_tensor(Elope.model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tensor = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_k_max_pooling2d_5/Mean:0' shape=(None, None) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7284, 4), dtype=int64, numpy=\n",
       "array([[0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0]], dtype=int64)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
