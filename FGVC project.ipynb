{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.resnet import ResNet50\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "\n",
    "np.random.seed(0)\n",
    "tf.random.set_seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(train, directory):\n",
    "    \n",
    "    # This function loads the images, resizes them and puts them into an array\n",
    "    \n",
    "    img_size = 224\n",
    "    train_image = []\n",
    "    for name in train['image_id']:\n",
    "        path = directory + 'images/' + name + '.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        train_image.append(img)\n",
    "    train_image_array = np.array(train_image)\n",
    "    \n",
    "    return train_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_augmented(directory):\n",
    "    \n",
    "    # This function loads the augmented images and the augmented csv file\n",
    "    df_train = pd.read_csv(directory + 'augmented.csv')\n",
    "    \n",
    "    train_image = []\n",
    "    for name in df_train['image_id']:\n",
    "        path = directory + 'images_resized_augmented/' + name + '.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        train_image.append(img)\n",
    "    train_image_array = np.array(train_image)\n",
    "    \n",
    "    return  train_image_array, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/julen/OneDrive/Escritorio/IA/CS577-Deep-Learning/Project/'\n",
    "x_train, df_train = load_images_augmented(directory)\n",
    "df_test = pd.read_csv(directory + 'test.csv')\n",
    "x_test = load_images(df_test, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Normalize -> We don't because of memory issues. A float32 occupies 4x more than the original uint8\n",
    "# x_train = x_train / 255.0 \n",
    "# x_test =  x_test / 255.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split training set into train and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of x_train:  (7284, 224, 224, 3)\n",
      "Size of x_val:  (1821, 224, 224, 3)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = df_train[['healthy', 'multiple_diseases', 'rust', 'scab']].to_numpy()\n",
    "\n",
    "x_train_original, y_train_original = shuffle(x_train, y_train)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_original, y_train_original, test_size = 0.2, random_state = 2020)\n",
    "\n",
    "print('Size of x_train: ', x_train.shape)\n",
    "print('Size of x_val: ', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, x_test, name):\n",
    "    x_pred = model.predict(x_test)\n",
    "    df_test['healthy'] = x_pred[:,0]\n",
    "    df_test['multiple_diseases'] = x_pred[:,1]\n",
    "    df_test['rust'] = x_pred[:,2]\n",
    "    df_test['scab'] = x_pred[:,3]\n",
    "    df_test.to_csv(name, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2070,  346, 2495, 2373], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#See distribution of samples\n",
    "y_train.sum(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First approach: Batch Confusion Norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plot(history):\n",
    "\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(model, x_test, y_test):\n",
    "    score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ASSP(output, act_name): # TODO: Check if it is well built. Not like in paper. Maybe different image size?\n",
    "    ASSP = Conv2D(filters = 2048, kernel_size = (1,1))(output)\n",
    "    ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 2)(ASSP)\n",
    "#     ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 4)(ASSP)\n",
    "    # ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 6)(ASSP)\n",
    "    # ASSP = Conv2D(filters = 2048, kernel_size = (3,3), dilation_rate = 7)(ASSP)\n",
    "\n",
    "    ASSP = Activation(act_name)(ASSP)\n",
    "    \n",
    "    return ASSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def compute_bcn_loss(y_pred):\n",
    "\n",
    "    \n",
    "    matrix = K.dot(K.transpose(y_pred), y_pred)\n",
    "    eigenvalues, eigenvectors = tf.linalg.eigh(matrix)\n",
    "    bcn_loss = K.sum(eigenvalues) \n",
    "    \n",
    "    print(eigenvalues)\n",
    "    print(type(eigenvalues))\n",
    "    print(type(y_pred))\n",
    "    print('y_pred shape: ', y_pred.shape)\n",
    "    print('yPred.transpose shape: ', K.transpose(y_pred).shape)\n",
    "    print('matrix shape: ', matrix.shape)\n",
    "    print('eigenvalues shape: ', eigenvalues.shape)\n",
    "    \n",
    "    print()\n",
    "    \n",
    "    \n",
    "    \n",
    "    return bcn_loss\n",
    "\n",
    "\n",
    "\n",
    "def custom_loss(y_true, y_pred):\n",
    "    \n",
    "    categorical_crossentropy = losses.categorical_crossentropy(y_true, y_pred)\n",
    "    \n",
    "    BCN_loss = compute_bcn_loss(y_pred)\n",
    "    \n",
    "    loss_total = categorical_crossentropy + BCN_loss\n",
    "    \n",
    "    return loss_total\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"loss/dense_2_loss/custom_loss/SelfAdjointEigV2:0\", shape=(4,), dtype=float32)\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "<class 'tensorflow.python.framework.ops.Tensor'>\n",
      "y_pred shape:  (None, 4)\n",
      "yPred.transpose shape:  (4, None)\n",
      "matrix shape:  (4, 4)\n",
      "eigenvalues shape:  (4,)\n",
      "\n",
      "Model: \"model_1\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 224, 224, 3)  0                                            \n",
      "__________________________________________________________________________________________________\n",
      "resnet50 (Model)                (None, 7, 7, 2048)   23587712    input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_1 (Conv2D)               (None, 7, 7, 2048)   4196352     resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_3 (Conv2D)               (None, 7, 7, 2048)   4196352     resnet50[1][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_2 (Conv2D)               (None, 3, 3, 2048)   37750784    conv2d_1[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "conv2d_4 (Conv2D)               (None, 3, 3, 2048)   37750784    conv2d_3[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_1 (Activation)       (None, 3, 3, 2048)   0           conv2d_2[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "activation_2 (Activation)       (None, 3, 3, 2048)   0           conv2d_4[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "multiply_1 (Multiply)           (None, 3, 3, 2048)   0           activation_1[0][0]               \n",
      "                                                                 activation_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "flatten_1 (Flatten)             (None, 18432)        0           multiply_1[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 256)          4718848     flatten_1[0][0]                  \n",
      "__________________________________________________________________________________________________\n",
      "dense_2 (Dense)                 (None, 4)            1028        dense_1[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 112,201,860\n",
      "Trainable params: 112,148,740\n",
      "Non-trainable params: 53,120\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "inputs = Input(shape = (224, 224, 3))\n",
    "out = model_resnet(inputs)\n",
    "features = ASSP(out, 'relu')\n",
    "attention = ASSP(out, 'sigmoid')\n",
    "tensor_total = Multiply()([features, attention])\n",
    "tensor_total_flat = Flatten()(tensor_total)\n",
    "tensor_total_flat = Dense(256, activation = 'relu')(tensor_total_flat) # TODO: Check if it should be relu or another\n",
    "tensor_total_flat = Dense(4, activation = 'softmax')(tensor_total_flat)\n",
    "\n",
    "model = Model(inputs=inputs, outputs= tensor_total_flat)\n",
    "\n",
    "model.compile(optimizer = 'adam', loss = custom_loss, metrics = ['categorical_accuracy'])\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7284 samples, validate on 1821 samples\n",
      "Epoch 1/3\n"
     ]
    },
    {
     "ename": "ResourceExhaustedError",
     "evalue": " OOM when allocating tensor with shape[3,3,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node mul_1092 (defined at C:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_50206]\n\nFunction call stack:\nkeras_scratch_graph\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mResourceExhaustedError\u001b[0m                    Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-15-097070a4f8de>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvalidation_data\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_freq, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[0;32m   1237\u001b[0m                                         \u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msteps_per_epoch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1238\u001b[0m                                         \u001b[0mvalidation_steps\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mvalidation_steps\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1239\u001b[1;33m                                         validation_freq=validation_freq)\n\u001b[0m\u001b[0;32m   1240\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1241\u001b[0m     def evaluate(self,\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\engine\\training_arrays.py\u001b[0m in \u001b[0;36mfit_loop\u001b[1;34m(model, fit_function, fit_inputs, out_labels, batch_size, epochs, verbose, callbacks, val_function, val_inputs, shuffle, initial_epoch, steps_per_epoch, validation_steps, validation_freq)\u001b[0m\n\u001b[0;32m    194\u001b[0m                     \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mins_batch\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtoarray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfit_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m                 \u001b[0mouts\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mto_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m                 \u001b[1;32mfor\u001b[0m \u001b[0ml\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mo\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mout_labels\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mouts\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, inputs)\u001b[0m\n\u001b[0;32m   3725\u001b[0m         \u001b[0mvalue\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mmath_ops\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtensor\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3726\u001b[0m       \u001b[0mconverted_inputs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 3727\u001b[1;33m     \u001b[0moutputs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_graph_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mconverted_inputs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3728\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3729\u001b[0m     \u001b[1;31m# EagerTensor.numpy() will often make a copy to ensure memory safety.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1549\u001b[0m       \u001b[0mTypeError\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mFor\u001b[0m \u001b[0minvalid\u001b[0m \u001b[0mpositional\u001b[0m\u001b[1;33m/\u001b[0m\u001b[0mkeyword\u001b[0m \u001b[0margument\u001b[0m \u001b[0mcombinations\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1550\u001b[0m     \"\"\"\n\u001b[1;32m-> 1551\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1552\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1553\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_impl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_impl\u001b[1;34m(self, args, kwargs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1589\u001b[0m       raise TypeError(\"Keyword arguments {} unknown. Expected {}.\".format(\n\u001b[0;32m   1590\u001b[0m           list(kwargs.keys()), list(self._arg_keywords)))\n\u001b[1;32m-> 1591\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1592\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1593\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1690\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[1;32m-> 1692\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[0;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[0;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 545\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    546\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     65\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     66\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 67\u001b[1;33m     \u001b[0msix\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     68\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mTypeError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     69\u001b[0m     keras_symbolic_tensors = [\n",
      "\u001b[1;32mC:\\Programs_julen\\Anaconda3\\lib\\site-packages\\six.py\u001b[0m in \u001b[0;36mraise_from\u001b[1;34m(value, from_value)\u001b[0m\n",
      "\u001b[1;31mResourceExhaustedError\u001b[0m:  OOM when allocating tensor with shape[3,3,2048,2048] and type float on /job:localhost/replica:0/task:0/device:GPU:0 by allocator GPU_0_bfc\n\t [[node mul_1092 (defined at C:\\Programs_julen\\Anaconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3009) ]]\nHint: If you want to see a list of allocated tensors when OOM happens, add report_tensor_allocations_upon_oom to RunOptions for current allocation info.\n [Op:__inference_keras_scratch_graph_50206]\n\nFunction call stack:\nkeras_scratch_graph\n"
     ]
    }
   ],
   "source": [
    "model.fit(x_train, y_train, batch_size = 2, epochs = 3, validation_data = (x_val, y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: In order to set the batch size and each batch with different labels, we should use fit_generator or train_on_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Simple model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(x_train, y_train, batch_size, epochs, verbose = True):\n",
    "    \n",
    "    \n",
    "    ##model building\n",
    "    model = Sequential()\n",
    "\n",
    "    model.add(Conv2D(32, kernel_size=(3, 3),\n",
    "                     activation='relu', input_shape = (224, 224, 3)))\n",
    "\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(128, kernel_size=(3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(Dropout(0.25))\n",
    "\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "    \n",
    "    model.add(Conv2D(1024, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "\n",
    "    model.add(Dense(128, activation='relu'))\n",
    "\n",
    "    model.add(Dense(4, activation='softmax'))\n",
    "    \n",
    "\n",
    "    print(model.summary())\n",
    "    start_time = time.time()\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam' , metrics = ['categorical_accuracy'])\n",
    "    history = model.fit(x = x_train, y = y_train, validation_data = (x_val, y_val), batch_size = batch_size, epochs = epochs, verbose = verbose)\n",
    "    print(\"--- %s seconds ---\" % (time.time() - start_time))\n",
    "    \n",
    "    return (model, history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_6 (Conv2D)            (None, 222, 222, 32)      896       \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 220, 220, 64)      18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_6 (MaxPooling2 (None, 110, 110, 64)      0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 108, 108, 128)     73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_7 (MaxPooling2 (None, 54, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 54, 54, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 52, 52, 512)       590336    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_8 (MaxPooling2 (None, 26, 26, 512)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 24, 24, 1024)      4719616   \n",
      "_________________________________________________________________\n",
      "max_pooling2d_9 (MaxPooling2 (None, 12, 12, 1024)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_10 (MaxPooling (None, 6, 6, 1024)        0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 36864)             0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               4718720   \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 4)                 516       \n",
      "=================================================================\n",
      "Total params: 10,122,436\n",
      "Trainable params: 10,122,436\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Train on 7284 samples, validate on 1821 samples\n",
      "Epoch 1/1\n",
      "7284/7284 [==============================] - 115s 16ms/step - loss: 2.8426 - categorical_accuracy: 0.3471 - val_loss: 1.2473 - val_categorical_accuracy: 0.3773\n",
      "--- 116.12400031089783 seconds ---\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'acc'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-67daff237a33>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mhistory\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbuild_model\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mbatch_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m16\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mmodel_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel_old\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-5ccc60850d14>\u001b[0m in \u001b[0;36mmodel_plot\u001b[1;34m(history)\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mmodel_plot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'val_acc'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'Model accuracy'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'acc'"
     ]
    }
   ],
   "source": [
    "model_old, history = build_model(x_train, y_train, batch_size = 16, epochs = 1)\n",
    "model_plot(history)\n",
    "print_score(model_old, x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_test['healthy'] = x_pred[:,0]\n",
    "df_test['multiple_diseases'] = x_pred[:,1]\n",
    "df_test['rust'] = x_pred[:,2]\n",
    "df_test['scab'] = x_pred[:,3]\n",
    "df_test.to_csv('FGVC_submission_.csv', index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>healthy</th>\n",
       "      <th>multiple_diseases</th>\n",
       "      <th>rust</th>\n",
       "      <th>scab</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>Test_0</td>\n",
       "      <td>0.205251</td>\n",
       "      <td>0.045785</td>\n",
       "      <td>0.368202</td>\n",
       "      <td>0.380761</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>Test_1</td>\n",
       "      <td>0.231453</td>\n",
       "      <td>0.047746</td>\n",
       "      <td>0.345878</td>\n",
       "      <td>0.374923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>Test_2</td>\n",
       "      <td>0.223623</td>\n",
       "      <td>0.071676</td>\n",
       "      <td>0.342834</td>\n",
       "      <td>0.361867</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>Test_3</td>\n",
       "      <td>0.308657</td>\n",
       "      <td>0.102331</td>\n",
       "      <td>0.295816</td>\n",
       "      <td>0.293197</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>Test_4</td>\n",
       "      <td>0.230924</td>\n",
       "      <td>0.044711</td>\n",
       "      <td>0.364794</td>\n",
       "      <td>0.359571</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1816</td>\n",
       "      <td>Test_1816</td>\n",
       "      <td>0.220903</td>\n",
       "      <td>0.053204</td>\n",
       "      <td>0.343984</td>\n",
       "      <td>0.381910</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1817</td>\n",
       "      <td>Test_1817</td>\n",
       "      <td>0.273419</td>\n",
       "      <td>0.108838</td>\n",
       "      <td>0.302602</td>\n",
       "      <td>0.315141</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1818</td>\n",
       "      <td>Test_1818</td>\n",
       "      <td>0.324635</td>\n",
       "      <td>0.099112</td>\n",
       "      <td>0.290823</td>\n",
       "      <td>0.285430</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1819</td>\n",
       "      <td>Test_1819</td>\n",
       "      <td>0.337709</td>\n",
       "      <td>0.099663</td>\n",
       "      <td>0.284925</td>\n",
       "      <td>0.277703</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1820</td>\n",
       "      <td>Test_1820</td>\n",
       "      <td>0.201312</td>\n",
       "      <td>0.038694</td>\n",
       "      <td>0.375525</td>\n",
       "      <td>0.384469</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1821 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       image_id   healthy  multiple_diseases      rust      scab\n",
       "0        Test_0  0.205251           0.045785  0.368202  0.380761\n",
       "1        Test_1  0.231453           0.047746  0.345878  0.374923\n",
       "2        Test_2  0.223623           0.071676  0.342834  0.361867\n",
       "3        Test_3  0.308657           0.102331  0.295816  0.293197\n",
       "4        Test_4  0.230924           0.044711  0.364794  0.359571\n",
       "...         ...       ...                ...       ...       ...\n",
       "1816  Test_1816  0.220903           0.053204  0.343984  0.381910\n",
       "1817  Test_1817  0.273419           0.108838  0.302602  0.315141\n",
       "1818  Test_1818  0.324635           0.099112  0.290823  0.285430\n",
       "1819  Test_1819  0.337709           0.099663  0.284925  0.277703\n",
       "1820  Test_1820  0.201312           0.038694  0.375525  0.384469\n",
       "\n",
       "[1821 rows x 5 columns]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Second approach: Localization, Pooling and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer, InputSpec\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GlobalKMaxPooling2D(Layer): #Inherits the properties of Layer class\n",
    "    \"\"\"K Max Pooling operation for spatial data.\n",
    "    \n",
    "    # Arguments\n",
    "        \n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` (default) or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `\"channels_first\"`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be `\"channels_last\"`.\n",
    "            \n",
    "        K: An Integer,\n",
    "            states the number of selected maximal values over which the\n",
    "            average is going to be computed.\n",
    "            \n",
    "            \n",
    "    # Input shape\n",
    "    \n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, rows, cols, channels)`\n",
    "            \n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, rows, cols)`\n",
    "            \n",
    "    # Output shape\n",
    "    \n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, pooled_rows, pooled_cols, channels)`\n",
    "            \n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, pooled_rows, pooled_cols)`    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, data_format=None, k = 10, **kwargs):\n",
    "        super(GlobalKMaxPooling2D, self).__init__(**kwargs)\n",
    "        self.data_format = K.normalize_data_format(data_format)\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'data_format': self.data_format, 'k' : self.k}\n",
    "        base_config = super(GlobalKMaxPooling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if self.data_format == 'channels_last':\n",
    "            # Here first sort\n",
    "            # Then take K maximum values\n",
    "            # Then average them\n",
    "            k = self.k\n",
    "\n",
    "            input_reshaped = tf.reshape(inputs, [tf.shape(inputs)[0], -1, tf.shape(inputs)[3]])\n",
    "            input_reshaped = tf.reshape(input_reshaped, [tf.shape(input_reshaped)[0], tf.shape(input_reshaped)[2], tf.shape(input_reshaped)[1]])\n",
    "            top_k = tf.math.top_k(input_reshaped, k=k, sorted = True, name = None)[0]\n",
    "            mean = tf.keras.backend.mean(top_k, axis = 2)\n",
    "            #assert ((input_reshaped.get_shape()[0], input_reshaped.get_shape()[-1]) == mean.get_shape())\n",
    "        \n",
    "        return mean\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n",
    "               lr_min=0.00001, lr_rampup_epochs=5, \n",
    "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "    lr_max = lr_max #* strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) *\\\n",
    "                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n",
    "                                - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    return lrfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfn = build_lrfn()\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'EmbeddingLayerLoss' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-53-b273137d100b>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# model.add(Dense(128, activation = 'relu'))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mDense\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m4\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mactivation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'softmax'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moptimizer\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mEmbeddingLayerLoss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmetrics\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'categorical_accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'EmbeddingLayerLoss' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(model_resnet)\n",
    "model.add(GlobalKMaxPooling2D(data_format = 'channels_last' , k = 4))\n",
    "# model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = EmbeddingLayerLoss(model.layers[2]), metrics = ['categorical_accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 431,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    }
   ],
   "source": [
    "from keras.applications.resnet import ResNet50\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "tf.compat.v1.enable_eager_execution()\n",
    "print(tf.executing_eagerly())\n",
    "    \n",
    "class ElopeModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Model\n",
    "        self.model = self.build_model()\n",
    "        # Print a model summary\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = 'Adam'\n",
    "        \n",
    "        # Have to convert to tensor??\n",
    "        self.loss_parameters = {'means' : 0, 'lr' : 0.5, 'landa' : 2.0,\n",
    "                               'gamma' : 16.0, 'm' : 0.75}\n",
    "\n",
    "        \n",
    "        #Loss Function\n",
    "        self.loss_func = self.model_loss()\n",
    "        \n",
    "        self.compile()\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        input_shape = (224, 224, 3)\n",
    "        model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(model_resnet)\n",
    "        model.add(GlobalKMaxPooling2D(data_format = 'channels_last' , k = 4))\n",
    "        model.add(Dense(4, activation = 'softmax'))\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def calculate_within_class_loss(self, y_true, y_pred, fx_tensor): #fx_tensor = Output of layer\n",
    "       \n",
    "        mean_tensor = self.loss_parameters['means']\n",
    "        alpha = self.loss_parameters['lr']\n",
    "\n",
    "        dimension_tensor = K.shape(fx_tensor)[1]\n",
    "        num_classes_tensor = K.shape(y_true)[1]\n",
    "        if isinstance(mean_tensor, int): # Initialize them if they are not yet initialized\n",
    "            mean_tensor = tf.zeros([num_classes, dimension], dtype = tf.dtypes.float64)\n",
    "        num_samples_tensor = K.shape(fx_tensor)[0]\n",
    "        \n",
    "        # Ensure they are float 64\n",
    "        fx_tensor = tf.cast(fx_tensor, dtype = tf.dtypes.float64)\n",
    "        \n",
    "\n",
    "        fx_expanded = tf.broadcast_to(tf.expand_dims(fx_tensor, axis = -1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "        y_true_expanded = tf.broadcast_to(tf.expand_dims(y_true_tensor, axis = 1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "        mean_expanded = tf.broadcast_to(tf.expand_dims(mean_tensor, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "\n",
    "        mean_expanded = tf.transpose(mean_expanded, perm = [0,2,1])\n",
    "\n",
    "        up = tf.reduce_sum(tf.multiply(tf.subtract(mean_expanded, fx_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)), axis = 0)\n",
    "        y_true_cut = tf.reduce_sum(y_true_tensor, axis = 0)\n",
    "        down = tf.add(y_true_cut, tf.constant(1))\n",
    "        down = tf.cast(down, dtype = tf.dtypes.float64)\n",
    "        delta = tf.divide(up, down)\n",
    "        delta = tf.transpose(delta)\n",
    "\n",
    "        mean_new = tf.subtract(mean_tensor, tf.scalar_mul(tf.constant(alpha, dtype = tf.dtypes.float64), delta))\n",
    "\n",
    "        # Now calculate the loss\n",
    "        mean_new_expanded = tf.broadcast_to(tf.expand_dims(mean_new, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "        mean_new_expanded = tf.transpose(mean_new_expanded,  perm = [0,2,1]) \n",
    "        inside = tf.reduce_sum(tf.multiply(tf.subtract(fx_expanded, mean_new_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)), axis = 2)\n",
    "\n",
    "\n",
    "        tot = tf.reduce_sum(tf.multiply(inside, inside)) # Apply the norm\n",
    "        down = tf.multiply(tf.constant(2), num_samples_tensor)\n",
    "        tot = tf.divide(tot, tf.cast(down, dtype = tf.dtypes.float64))\n",
    "\n",
    "        mean_tensor = mean_new\n",
    "        \n",
    "        # N = number of samples in the batch\n",
    "        # f(xn) -> Dimension = 2048 (not the batch because it it xn, only 1 sample)\n",
    "        # class means -> should be the same as f(xn) x number of classes\n",
    "        # Therefore ||f(xn) - U(cn)||^2 will be a number, because f(xn) is a vector, not a matrix\n",
    "\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_between_class_loss(self, y_true, y_pred, layer_output):\n",
    "        # Layer there or in self??\n",
    "        class_means = self.loss_parameters['means']\n",
    "        gamma = self.loss_parameters['gamma']\n",
    "        m = self.loss_parameters['m']\n",
    "        # m -> Margin\n",
    "        # P -> class-pairs in the current batch\n",
    "        # |P| -> Cardinality of P, number of elements in the set\n",
    "        \n",
    "#         bc_loss = \n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"\" Wrapper function which calculates auxiliary values for the complete loss function.\n",
    "         Returns a *function* which calculates the complete loss given only the input and target output \"\"\"\n",
    "        # This part has to be developed\n",
    "        \n",
    "        #Within class loss\n",
    "        within_class_loss_func = self.calculate_within_class_loss\n",
    "        \n",
    "        # Between class loss\n",
    "        between_class_loss_func = self.calculate_between_class_loss\n",
    "        \n",
    "        lay_out = self.model.layers[-2].output\n",
    "        \n",
    "        \n",
    "        def ElopeLoss(y_true, y_pred):\n",
    "            \n",
    "            # Within Class loss has to be computed first, in order to get the new class means updated\n",
    "            \n",
    "            within_class_loss = within_class_loss_func(y_true, y_pred, lay_out)\n",
    "            \n",
    "            between_class_loss = between_class_loss_func(y_true, y_pred, lay_out)\n",
    "            \n",
    "            cat_cross_loss = categorical_crossentropy(y_true, y_pred)\n",
    "            \n",
    "            model_loss = cat_cross_loss + within_class_loss #+ between_class_loss\n",
    "            \n",
    "            return model_loss\n",
    "        \n",
    "        \n",
    "        return ElopeLoss\n",
    "    \n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\" Compiles the Keras model. Includes metrics to differentiate between the two main loss terms \"\"\"\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        self.model.compile(optimizer = self.optimizer, loss = self.loss_func,\n",
    "                          metrics = [categorical_crossentropy]) # Here put the two losses better\n",
    "        print('Model Compiled!')\n",
    "        \n",
    "    \n",
    "    def load_trained_weights(self, weights):\n",
    "        \"\"\" Loads weights of a pre-trained model. 'weights' is path to h5 model\\weights file\"\"\"\n",
    "        self.model.load_weights(weights)\n",
    "        print('Weights from {} loaded successfully'.format(weights))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 432,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eager execution: True\n",
      "Eager execution: True\n",
      "Model: \"sequential_31\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "resnet50 (Model)             (None, 7, 7, 2048)        23587712  \n",
      "_________________________________________________________________\n",
      "global_k_max_pooling2d_31 (G (None, 2048)              0         \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 4)                 8196      \n",
      "=================================================================\n",
      "Total params: 23,595,908\n",
      "Trainable params: 23,542,788\n",
      "Non-trainable params: 53,120\n",
      "_________________________________________________________________\n",
      "Eager execution: True\n",
      "Model Compiled!\n"
     ]
    }
   ],
   "source": [
    "Elope = ElopeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'tensorflow' has no attribute 'VERSION'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-33-1b3b6cee90b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"TensorFlow version: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mVERSION\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Eager execution: {}\"\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'tensorflow' has no attribute 'VERSION'"
     ]
    }
   ],
   "source": [
    "print(\"TensorFlow version: {}\".format(tf.VERSION))\n",
    "print(\"Eager execution: {}\".format(tf.executing_eagerly()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.executing_eagerly()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 7284 samples, validate on 1821 samples\n",
      "Epoch 1/1\n",
      "\n",
      "Epoch 00001: LearningRateScheduler reducing learning rate to 1e-05.\n",
      "7284/7284 [==============================] - 471s 65ms/step - loss: 1.6179 - categorical_crossentropy: 1.6179 - val_loss: 0.9083 - val_categorical_crossentropy: 0.9083\n"
     ]
    }
   ],
   "source": [
    "history = Elope.model.fit(x_train, y_train, batch_size = 12, epochs = 1, \n",
    "                    validation_data = (x_val, y_val), callbacks = [lr_schedule])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Steps needed for calculating the Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to specify some random values and convert them to tensors\n",
    "\n",
    "f_x = np.random.rand(3,2048)\n",
    "mean = np.random.rand(4,2048)\n",
    "y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0]])\n",
    "\n",
    "fx_tensor = tf.convert_to_tensor(f_x)\n",
    "mean_tensor = tf.convert_to_tensor(mean)\n",
    "y_true_tensor = tf.convert_to_tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2048"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to get the necessary slices from the tensors\n",
    "\n",
    "fx_tensor_slice = tf.slice(f_x, [1,0], [1, tf.shape(fx_tensor)[1].numpy()]) #Begin, size\n",
    "y_true_slice = tf.slice(y_true, [1,0], [1,4])\n",
    "class_number = np.argmax(y_true_slice.numpy()) # Class goes from 0 to 3, in order to fit better into slice method\n",
    "mean_slice = tf.slice(mean_tensor, [class_number, 0], [1,tf.shape(mean_tensor)[1].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_x shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "mean shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "y_true shape:  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('f_x shape: ', tf.shape(fx_tensor_slice))\n",
    "print('mean shape: ', tf.shape(mean_slice) )\n",
    "print('y_true shape: ', tf.shape(y_true_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 2048), dtype=float64, numpy=\n",
       "array([[ 0.19938234,  0.00816488, -0.6013304 , ...,  0.21713867,\n",
       "        -0.3828902 , -0.39345389]])>"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean_slice - fx_tensor_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.81525007, 0.79134176, 0.44398508, ..., 0.6280034 , 0.2294754 ,\n",
       "        0.76706728],\n",
       "       [0.43227968, 0.8426107 , 0.91554432, ..., 0.17903316, 0.97670381,\n",
       "        0.77232114],\n",
       "       [0.14680833, 0.40305313, 0.5662111 , ..., 0.21604621, 0.48619845,\n",
       "        0.07001679]])"
      ]
     },
     "execution_count": 173,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(fx_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate loss with graph mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "13.666666666666664\n",
      "[[-1.75       -0.75       -2.75       -2.25      ]\n",
      " [ 3.83333333  4.83333333  6.66666667  5.83333333]]\n",
      "\n",
      "---------------- NOW THE LOSS ---------------\n",
      "\n",
      "\n",
      "3.550925925925926\n"
     ]
    }
   ],
   "source": [
    "alpha = 0.5\n",
    "num_classes = mean_tensor.shape[0]\n",
    "num_samples_in_batch = fx_tensor.shape[0]\n",
    "dimensions = mean_tensor.shape[1]\n",
    "\n",
    "# print('mean_tensor shape: ', mean_tensor.shape)\n",
    "# print('fx_tensor shape: ', fx_tensor.shape)\n",
    "# print('y_true shape: ', y_true.shape)\n",
    "\n",
    "dimension_tensor = K.shape(fx_tensor)[1]\n",
    "num_classes_tensor = K.shape(y_true)[1]\n",
    "num_samples_tensor = K.shape(y_true)[0]\n",
    "\n",
    "fx_expanded = tf.broadcast_to(tf.expand_dims(fx_tensor, axis = -1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "y_true_expanded = tf.broadcast_to(tf.expand_dims(y_true_tensor, axis = 1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "mean_expanded = tf.broadcast_to(tf.expand_dims(mean_tensor, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "# print(mean_expanded.numpy())\n",
    "mean_expanded = tf.transpose(mean_expanded, perm = [0,2,1])\n",
    "# mean_expanded = tf.reshape(mean_expanded, [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "\n",
    "# print('fx_expanded shape: ', fx_expanded.shape)\n",
    "# print('y_true_expanded shape: ', y_true_expanded.shape)\n",
    "# print('mean_expanded shape: ', mean_expanded.shape)\n",
    "# print()\n",
    "# print('fx_expanded: ', fx_expanded.numpy())\n",
    "# print()\n",
    "# print('y_true_expanded: ', y_true_expanded.numpy())\n",
    "# print()\n",
    "# print('mean_expanded: ', mean_expanded.numpy())\n",
    "# print()\n",
    "# print('result of subtraction: ', tf.multiply(tf.subtract(mean_expanded, fx_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)).numpy())\n",
    "\n",
    "# up = (mean_expanded - fx_expanded) * y_true_expanded (acting as mask)\n",
    "up = tf.reduce_sum(tf.multiply(tf.subtract(mean_expanded, fx_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)), axis = 0)\n",
    "# print('upper part: ', up.numpy())\n",
    "# for the part below:\n",
    "y_true_cut = tf.reduce_sum(y_true_tensor, axis = 0)\n",
    "down = tf.add(y_true_cut, tf.constant(1))\n",
    "down = tf.cast(down, dtype = tf.dtypes.float64)\n",
    "delta = tf.divide(up, down)\n",
    "\n",
    "# print(mean_tensor.numpy() - 0.5 * delta.numpy())\n",
    "delta = tf.transpose(delta)\n",
    "# print(delta.numpy())\n",
    "# print(mean_tensor.numpy())\n",
    "# print('delta shape: ', delta.shape)\n",
    "# print('mean shape:', mean_tensor.shape)\n",
    "mean_new = tf.subtract(mean_tensor, tf.scalar_mul(tf.constant(alpha, dtype = tf.dtypes.float64), delta))\n",
    "\n",
    "mean_new_sum = np.sum(mean_new.numpy())\n",
    "print()\n",
    "print(mean_new_sum)\n",
    "print(mean_new.numpy())\n",
    "print()\n",
    "print('---------------- NOW THE LOSS ---------------')\n",
    "print()\n",
    "\n",
    "# Now calculate the loss\n",
    "mean_new_expanded = tf.broadcast_to(tf.expand_dims(mean_new, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "mean_new_expanded = tf.transpose(mean_new_expanded,  perm = [0,2,1]) \n",
    "# print(mean_new_expanded.numpy())\n",
    "print()\n",
    "# mean_new_expanded = tf.reshape(mean_new_expanded, [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "inside = tf.reduce_sum(tf.multiply(tf.subtract(fx_expanded, mean_new_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)), axis = 2)\n",
    "\n",
    "# print(inside.shape)\n",
    "# print()\n",
    "# print(inside.numpy())\n",
    "tot = tf.reduce_sum(tf.multiply(inside, inside)) # Apply the norm\n",
    "down = tf.multiply(tf.constant(2), num_samples_tensor)\n",
    "tot = tf.divide(tot, tf.cast(down, dtype = tf.dtypes.float64))\n",
    "# print(tot.shape)\n",
    "print(tot.numpy())\n",
    "\n",
    "# mean_tensor = mean_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement it in Numpy to see it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to specify some random values and convert them to tensors\n",
    "\n",
    "# f_x = np.random.rand(6,2048) \n",
    "# mean = np.random.rand(4,2048) \n",
    "# y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0], [0,0,0,1], [1,0,0,0], [0,1,0,0]])\n",
    "\n",
    "f_x = np.array([[2,3,4,5], [-1, 0, -2, -3], [5,6, 8, 6]], dtype = 'float64')\n",
    "mean = np.array([[-2, -1, -3, -2], [4, 5, 7, 6]], dtype = 'float64')\n",
    "y_true = np.array([[0,1], [1,0], [0,1]])\n",
    "\n",
    "fx_tensor = tf.convert_to_tensor(f_x, dtype = tf.dtypes.float64)\n",
    "mean_tensor = tf.convert_to_tensor(mean, dtype = tf.dtypes.float64)\n",
    "y_true_tensor = tf.convert_to_tensor(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The new mean:  [[-1.31640625 -0.31640625 -2.31640625 -2.68359375]\n",
      " [ 3.59876543  4.59876543  6.19753086  5.59876543]]\n",
      "\n",
      "The final loss: 3.006169368040572\n"
     ]
    }
   ],
   "source": [
    "# This is well done!\n",
    "alpha = 0.5\n",
    "num_classes = 2\n",
    "num_samples_in_batch = 3\n",
    "dimension = 4\n",
    "\n",
    "# LOOP THROUGH CLASSES TO CALCULATE NEW MEAN\n",
    "for i in range(0, num_classes): # Loops throgh the classes\n",
    "\n",
    "    counter = 1 # to divide it\n",
    "    summ = np.zeros(dimension)  # Initialize class means \n",
    "    mean_i = mean[i]\n",
    "    for j in range(0, num_samples_in_batch): # Loops through the images in the batch  \n",
    "        y_true_sample = y_true[j] # Class of the i image\n",
    "        class_number = np.argmax(y_true_sample) \n",
    "        \n",
    "        if class_number == i: # if not, do nothing\n",
    "            counter += 1\n",
    "            summ = summ + (mean_i - f_x[j])\n",
    "        \n",
    "    total = summ / counter\n",
    "    mean[i] = mean_i - alpha*total \n",
    "    \n",
    "print('The new mean: ', mean)\n",
    "print()\n",
    "\n",
    "# Calculate the loss\n",
    "suma = 0\n",
    "for i in range(0, num_samples_in_batch):\n",
    "    y_true_sample = y_true[i] # Class of the i image\n",
    "    class_number = np.argmax(y_true_sample)\n",
    "    inside = f_x[i] - mean[class_number]\n",
    "    norm = np.linalg.norm(inside)\n",
    "    squared = np.square(norm)\n",
    "    suma = suma + np.square(np.linalg.norm(f_x[i] - mean[class_number]))\n",
    "loss = suma / (2*num_samples_in_batch)\n",
    "print('The final loss:', loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([5, 3, 4, 5])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mean[0] = np.array([5,3,4,5])\n",
    "mean[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "15.666666666666664"
      ]
     },
     "execution_count": 308,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "23/6 + 29/6  + 40/6 + 35/6 -7/4 -3/4 -11/4 -1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-5.5"
      ]
     },
     "execution_count": 309,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "-7/4 -3/4 -11/4 -1/4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.75, -0.75, -2.75, -2.25])"
      ]
     },
     "execution_count": 338,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array([-2, -1, -3, -2]) - np.array([-0.25, -0.25, -0.25, 0.25])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_tensor output: 4045.4047719107616\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "111.98640597369022"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Complete method\n",
    "\n",
    "# CANNOT USE NUMPY OR ARRAYS! Because it would make problems for backpropagation, as they are not in the graph.\n",
    "# Other way is to compute the backpropagation by ourselves.\n",
    "\n",
    "alpha = 0.5\n",
    "mean_new = [[], [], [], []] # 4 classes\n",
    "num_classes = tf.shape(mean_tensor)[0].numpy()\n",
    "num_samples_in_batch = tf.shape(fx_tensor)[0].numpy()\n",
    "dimension = tf.shape(mean_tensor)[1].numpy()\n",
    "\n",
    "# LOOP THROUGH CLASSES TO CALCULATE NEW MEAN\n",
    "for i in range(0, num_classes): # Loops throgh the classes\n",
    "\n",
    "    counter = 1 # to divide it\n",
    "    summ = tf.zeros(dimension, dtype = tf.dtypes.float64) # Initialize class means \n",
    "    # TODO: Initialize class means outside this function, as a parameter of self\n",
    "    mean_slice = tf.slice(mean_tensor, [i, 0], [1,dimension])\n",
    "\n",
    "    for j in range(0, num_samples_in_batch): # Loops through the images in the batch  \n",
    "        y_true_slice = tf.slice(y_true, [j,0], [1,4]) # Class of the i image\n",
    "        class_number = np.argmax(y_true_slice.numpy()) # Class goes from 0 to 3, in order to fit better into slice method.\n",
    "                                                        # Check if this is possible in the GPU\n",
    "\n",
    "        if class_number == i: # if not, do nothing\n",
    "            counter += 1\n",
    "            fx_tensor_slice = tf.slice(fx_tensor, [j,0], [1, dimension]) #Begin, size\n",
    "            summ = tf.add(summ, tf.subtract(mean_slice, fx_tensor_slice))\n",
    "        \n",
    "    total = tf.divide(summ, tf.constant(counter, dtype = tf.dtypes.float64))\n",
    "#     assert(np.sum(total.numpy()) == np.sum(summ.numpy() / counter))\n",
    "    \n",
    "    mean_new[i] = tf.subtract(mean_slice, tf.scalar_mul(tf.constant(alpha, dtype = tf.dtypes.float64), total))\n",
    "#     assert(np.sum(mean_new[i].numpy()) == np.sum(mean_slice.numpy() - alpha * total.numpy()))\n",
    "    \n",
    "# Assembly the new mean tensor\n",
    "mean_tensor_out = tf.concat([mean_new[0], mean_new[1], mean_new[2], mean_new[3]], axis = 0)\n",
    "print('mean_tensor output:', np.sum(mean_tensor_out.numpy()) )\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "# # CALCULATE LOSS\n",
    "loss = tf.Variable(0, dtype = tf.dtypes.float64)\n",
    "for i in range (0, num_samples_in_batch):\n",
    "    y_true_slice = tf.slice(y_true, [i,0], [1,4]) # Take the label of the ith sample\n",
    "    class_number = np.argmax(y_true_slice.numpy()) # Convert it to integer\n",
    "    mean_slice = tf.slice(mean_tensor, [class_number, 0], [1, dimension])\n",
    "    fx_tensor_slice = tf.slice(fx_tensor, [j,0], [1, dimension])\n",
    "    # Now do the subtract, square, sum and divide\n",
    "    assert(np.sum(tf.shape(mean_slice).numpy()) == np.sum(tf.shape(fx_tensor_slice).numpy()))\n",
    "    rest = tf.subtract(fx_tensor_slice, mean_slice)\n",
    "    norm = tf.square(tf.norm(rest))\n",
    "    loss = tf.add(loss, norm)\n",
    "    \n",
    "loss = tf.divide(loss, tf.Variable(2*num_samples_in_batch, dtype = tf.dtypes.float64))\n",
    "loss.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2048), dtype=float64, numpy=\n",
       "array([[0.81525007, 0.79134176, 0.44398508, ..., 0.6280034 , 0.2294754 ,\n",
       "        0.76706728],\n",
       "       [0.43227968, 0.8426107 , 0.91554432, ..., 0.17903316, 0.97670381,\n",
       "        0.77232114],\n",
       "       [0.14680833, 0.40305313, 0.5662111 , ..., 0.21604621, 0.48619845,\n",
       "        0.07001679]])>"
      ]
     },
     "execution_count": 182,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fx_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 3 required positional arguments: 'op', 'value_index', and 'dtype'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-ad72b0175e28>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m: __init__() missing 3 required positional arguments: 'op', 'value_index', and 'dtype'"
     ]
    }
   ],
   "source": [
    "tf.Tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'mean_tot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-faa0e8882ae0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mmean_tot\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'mean_tot' is not defined"
     ]
    }
   ],
   "source": [
    "mean_tot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "f_x shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "mean shape:  tf.Tensor([   1 2048], shape=(2,), dtype=int32)\n",
      "y_true shape:  tf.Tensor([1 4], shape=(2,), dtype=int32)\n"
     ]
    }
   ],
   "source": [
    "print('f_x shape: ', tf.shape(fx_tensor_slice))\n",
    "print('mean shape: ', tf.shape(mean_slice) )\n",
    "print('y_true shape: ', tf.shape(y_true_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_tensor_broadcasted = tf.broadcast_to(mean_tensor, [tf.shape(f_x_tensor)[0].numpy(), tf.shape(mean_tensor)[1].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 2048), dtype=float64, numpy=\n",
       "array([[ 0.52522555,  0.23903976,  0.00548459, ...,  0.69942552,\n",
       "         0.43518934,  0.03350187],\n",
       "       [ 0.18296376, -0.19688141, -0.83012728, ...,  0.15724875,\n",
       "        -0.33678901, -0.52118118],\n",
       "       [-0.10041236,  0.0561405 , -0.92195231, ...,  0.22434052,\n",
       "         0.45476284,  0.12300551]])>"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.math.subtract(mean_tensor_broadcasted, f_x_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tensor = tf.convert_to_tensor(y_true)\n",
    "y_true_tensor = tf.expand_dims(y_true_tensor, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3,), dtype=int32, numpy=array([3, 4, 1])>"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.shape(y_true_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 0, 0, 0]])"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_slice.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.argmax(y_true_slice.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_save(model, x_test, name = 'FGVC_submission_2_approach.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "lay_output = Elope.model.layers[-2].output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = tf.convert_to_tensor(Elope.model.predict(x_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true_tensor = tf.convert_to_tensor(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'global_k_max_pooling2d_5/Mean:0' shape=(None, None) dtype=float32>"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lay_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(7284, 4), dtype=int64, numpy=\n",
       "array([[0, 0, 1, 0],\n",
       "       [0, 0, 1, 0],\n",
       "       [1, 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, 1],\n",
       "       [0, 0, 1, 0],\n",
       "       [0, 1, 0, 0]], dtype=int64)>"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
