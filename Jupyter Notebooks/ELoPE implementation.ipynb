{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load images stored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.resnet import ResNet50\n",
    "import tensorflow as tf\n",
    "from keras import losses\n",
    "import keras.backend as K\n",
    "import tensorflow as tf\n",
    "from keras.models import model_from_json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(train, directory):\n",
    "    \n",
    "    # This function loads the images, resizes them and puts them into an array\n",
    "    \n",
    "    img_size = 224\n",
    "    train_image = []\n",
    "    for name in train['image_id']:\n",
    "        path = directory + 'images/' + name + '.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        img = cv2.resize(img, (img_size, img_size))\n",
    "        train_image.append(img)\n",
    "    train_image_array = np.array(train_image)\n",
    "    \n",
    "    return train_image_array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images_augmented(directory):\n",
    "    \n",
    "    # This function loads the augmented images and the augmented csv file\n",
    "    df_train = pd.read_csv(directory + 'augmented.csv')\n",
    "    train_image = []\n",
    "    for name in df_train['image_id']:\n",
    "        name = name.lower()\n",
    "        path = directory + 'images_resized_augmented/' + name + '.jpg'\n",
    "        img = cv2.imread(path)\n",
    "        train_image.append(img)\n",
    "    train_image_array = np.array(train_image)\n",
    "    \n",
    "    return  train_image_array, df_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/julen/OneDrive/Escritorio/IA/CS577-Deep-Learning/Project/'\n",
    "x_train, df_train = load_images_augmented(directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize\n",
    "x_train = x_train / 255.0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "y_train = df_train[['healthy', 'multiple_diseases', 'rust', 'scab']].to_numpy()\n",
    "\n",
    "x_train_original, y_train_original = shuffle(x_train, y_train)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_train_original, y_train_original, test_size = 0.2, random_state = 2020)\n",
    "\n",
    "print('Size of x_train: ', x_train.shape)\n",
    "print('Size of x_val: ', x_val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_and_save(model, x_test, name):\n",
    "    x_pred = model.predict(x_test)\n",
    "    df_test['healthy'] = x_pred[:,0]\n",
    "    df_test['multiple_diseases'] = x_pred[:,1]\n",
    "    df_test['rust'] = x_pred[:,2]\n",
    "    df_test['scab'] = x_pred[:,3]\n",
    "    df_test.to_csv(name, index = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_plot(history):\n",
    "\n",
    "    plt.plot(history.history['categorical_accuracy'])\n",
    "    plt.plot(history.history['val_categorical_accuracy'])\n",
    "    plt.title('Model accuracy')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'Validation'])\n",
    "    plt.show()\n",
    "\n",
    "    # Plot training & validation loss values\n",
    "    plt.plot(history.history['loss'])\n",
    "    plt.plot(history.history['val_loss'])\n",
    "    plt.title('Model loss')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.legend(['Train', 'validation'], loc='upper left')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_score(model, x_test, y_test):\n",
    "    score = model.evaluate(x_test, y_test, verbose = 0)\n",
    "    print('Test loss:', score[0])\n",
    "    print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Localization, Pooling and Embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import Layer, InputSpec\n",
    "from keras.legacy import interfaces\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class GlobalKMaxPooling2D(Layer): #Inherits the properties of Layer class\n",
    "    \"\"\"K Max Pooling operation for spatial data.\n",
    "    \n",
    "    # Arguments\n",
    "        \n",
    "        data_format: A string,\n",
    "            one of `\"channels_last\"` (default) or `\"channels_first\"`.\n",
    "            The ordering of the dimensions in the inputs.\n",
    "            `\"channels_last\"` corresponds to inputs with shape\n",
    "            `(batch, height, width, channels)` while `\"channels_first\"`\n",
    "            corresponds to inputs with shape\n",
    "            `(batch, channels, height, width)`.\n",
    "            It defaults to the `image_data_format` value found in your\n",
    "            Keras config file at `~/.keras/keras.json`.\n",
    "            If you never set it, then it will be `\"channels_last\"`.\n",
    "            \n",
    "        K: An Integer,\n",
    "            states the number of selected maximal values over which the\n",
    "            average is going to be computed.\n",
    "            \n",
    "            \n",
    "    # Input shape\n",
    "    \n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, rows, cols, channels)`\n",
    "            \n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, rows, cols)`\n",
    "            \n",
    "    # Output shape\n",
    "    \n",
    "        - If `data_format='channels_last'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, pooled_rows, pooled_cols, channels)`\n",
    "            \n",
    "        - If `data_format='channels_first'`:\n",
    "            4D tensor with shape:\n",
    "            `(batch_size, channels, pooled_rows, pooled_cols)`    \n",
    "    \n",
    "    \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    def __init__(self, data_format=None, k = 10, **kwargs):\n",
    "        super(GlobalKMaxPooling2D, self).__init__(**kwargs)\n",
    "        self.data_format = K.normalize_data_format(data_format)\n",
    "        self.input_spec = InputSpec(ndim=4)\n",
    "        self.k = k\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        if self.data_format == 'channels_last':\n",
    "            return (input_shape[0], input_shape[3])\n",
    "        else:\n",
    "            return (input_shape[0], input_shape[1])\n",
    "\n",
    "    def get_config(self):\n",
    "        config = {'data_format': self.data_format, 'k' : self.k}\n",
    "        base_config = super(GlobalKMaxPooling2D, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "    \n",
    "        \n",
    "    def call(self, inputs):\n",
    "        if self.data_format == 'channels_last':\n",
    "            # Here first sort\n",
    "            # Then take K maximum values\n",
    "            # Then average them\n",
    "            k = self.k\n",
    "\n",
    "            input_reshaped = tf.reshape(inputs, [tf.shape(inputs)[0], -1, tf.shape(inputs)[3]])\n",
    "            input_reshaped = tf.reshape(input_reshaped, [tf.shape(input_reshaped)[0], tf.shape(input_reshaped)[2], tf.shape(input_reshaped)[1]])\n",
    "            top_k = tf.math.top_k(input_reshaped, k=k, sorted = True, name = None)[0]\n",
    "            mean = tf.keras.backend.mean(top_k, axis = 2)\n",
    "            #assert ((input_reshaped.get_shape()[0], input_reshaped.get_shape()[-1]) == mean.get_shape())\n",
    "        \n",
    "        return mean\n",
    "\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_lrfn(lr_start=0.00001, lr_max=0.00005, \n",
    "               lr_min=0.00001, lr_rampup_epochs=5, \n",
    "               lr_sustain_epochs=0, lr_exp_decay=.8):\n",
    "    lr_max = lr_max #* strategy.num_replicas_in_sync\n",
    "\n",
    "    def lrfn(epoch):\n",
    "        if epoch < lr_rampup_epochs:\n",
    "            lr = (lr_max - lr_start) / lr_rampup_epochs * epoch + lr_start\n",
    "        elif epoch < lr_rampup_epochs + lr_sustain_epochs:\n",
    "            lr = lr_max\n",
    "        else:\n",
    "            lr = (lr_max - lr_min) *\\\n",
    "                 lr_exp_decay**(epoch - lr_rampup_epochs\\\n",
    "                                - lr_sustain_epochs) + lr_min\n",
    "        return lr\n",
    "    return lrfn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrfn = build_lrfn()\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import numpy as np\n",
    "from keras.applications.resnet import ResNet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (224, 224, 3)\n",
    "model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = Sequential()\n",
    "model.add(model_resnet)\n",
    "model.add(GlobalKMaxPooling2D(data_format = 'channels_last' , k = 4))\n",
    "# model.add(Dense(128, activation = 'relu'))\n",
    "model.add(Dense(4, activation = 'softmax'))\n",
    "model.compile(optimizer = 'adam', loss = EmbeddingLayerLoss(model.layers[2]), metrics = ['categorical_accuracy'])\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.layers[-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.applications.resnet import ResNet50\n",
    "from keras.losses import categorical_crossentropy\n",
    "import keras\n",
    "from keras import models, Sequential\n",
    "from keras.layers import Input, Dense, Activation, Flatten, Conv2D, Dropout, Multiply\n",
    "from keras.layers import AveragePooling2D, MaxPooling2D, GlobalMaxPooling2D, Reshape\n",
    "from keras.models import Model, Sequential\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "\n",
    "\n",
    "    \n",
    "class ElopeModel(object):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        # Model\n",
    "        self.model = self.build_model()\n",
    "        # Print a model summary\n",
    "        self.model.summary()\n",
    "        \n",
    "        # Optimizer\n",
    "        self.optimizer = 'Adam'\n",
    "        \n",
    "        # Have to convert to tensor??\n",
    "        self.loss_parameters = {'means' : 0, 'lr' : tf.constant(0.5,tf.float32), 'landa' : tf.constant(2.0,tf.float32),\n",
    "                               'gamma' : tf.constant(16.0,tf.float32), 'm' : tf.constant(0.75,tf.float32)}\n",
    "\n",
    "        \n",
    "        #Loss Function\n",
    "        self.loss_func = self.model_loss()\n",
    "        \n",
    "        self.compile()\n",
    "        \n",
    "        \n",
    "    def build_model(self):\n",
    "        \n",
    "        input_shape = (224, 224, 3)\n",
    "        model_resnet = ResNet50(include_top = False, weights = 'imagenet', input_shape = input_shape)\n",
    "        model = Sequential()\n",
    "        \n",
    "        model.add(model_resnet)\n",
    "        model.add(GlobalKMaxPooling2D(data_format = 'channels_last' , k = 4))\n",
    "        model.add(Dense(4, activation = 'softmax'))\n",
    "        tf.compat.v1.enable_eager_execution()\n",
    "        print('Eager execution:', tf.executing_eagerly())\n",
    "        \n",
    "        return model\n",
    "    \n",
    "    \n",
    "    def calculate_within_class_loss(self, y_true, y_pred, fx_tensor): #fx_tensor = Output of layer\n",
    "       \n",
    "        mean_tensor = self.loss_parameters['means']\n",
    "        alpha = self.loss_parameters['lr']\n",
    "\n",
    "        dimension_tensor = K.shape(fx_tensor)[1]\n",
    "        num_classes_tensor = K.shape(y_true)[1]\n",
    "        if isinstance(mean_tensor, int): # Initialize them if they are not yet initialized\n",
    "            mean_tensor = tf.zeros([num_classes_tensor, dimension_tensor], dtype = tf.dtypes.float32)\n",
    "        num_samples_tensor = K.shape(fx_tensor)[0]\n",
    "        \n",
    "        # Ensure they are float 32\n",
    "        fx_tensor = tf.cast(fx_tensor, dtype = tf.dtypes.float32)\n",
    "        \n",
    "\n",
    "        fx_expanded = tf.broadcast_to(tf.expand_dims(fx_tensor, axis = -1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "        y_true_expanded = tf.broadcast_to(tf.expand_dims(y_true, axis = 1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "        mean_expanded = tf.broadcast_to(tf.expand_dims(mean_tensor, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "\n",
    "        mean_expanded = tf.transpose(mean_expanded, perm = [0,2,1])\n",
    "\n",
    "        up = tf.reduce_sum(tf.multiply(tf.subtract(mean_expanded, fx_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float32)), axis = 0)\n",
    "        y_true_cut = tf.cast(tf.reduce_sum(y_true, axis = 0), dtype = tf.dtypes.float32)\n",
    "        down = tf.add(y_true_cut, tf.constant(1, dtype = tf.dtypes.float32))\n",
    "        delta = tf.divide(up, down)\n",
    "        delta = tf.transpose(delta)\n",
    "\n",
    "        mean_new = tf.subtract(mean_tensor, tf.scalar_mul(alpha, delta))\n",
    "\n",
    "        # Now calculate the loss\n",
    "        mean_new_expanded = tf.broadcast_to(tf.expand_dims(mean_new, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "        mean_new_expanded = tf.transpose(mean_new_expanded,  perm = [0,2,1]) \n",
    "        inside = tf.reduce_sum(tf.multiply(tf.subtract(fx_expanded, mean_new_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float32)), axis = 2)\n",
    "\n",
    "\n",
    "        tot = tf.reduce_sum(tf.multiply(inside, inside)) # Apply the norm\n",
    "        down = tf.multiply(tf.constant(2), num_samples_tensor)\n",
    "        loss = tf.divide(tot, tf.cast(down, dtype = tf.dtypes.float32))\n",
    "\n",
    "        self.loss_parameters['means'] = mean_new\n",
    "        \n",
    "        \n",
    "        # N = number of samples in the batch\n",
    "        # f(xn) -> Dimension = 2048 (not the batch because it it xn, only 1 sample)\n",
    "        # class means -> should be the same as f(xn) x number of classes\n",
    "        # Therefore ||f(xn) - U(cn)||^2 will be a number, because f(xn) is a vector, not a matrix\n",
    "\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    def calculate_between_class_loss(self, y_true, y_pred, fx_tensor):\n",
    "        # Layer there or in self??\n",
    "        mean_tensor = self.loss_parameters['means']\n",
    "        gamma = self.loss_parameters['gamma']\n",
    "        m = self.loss_parameters['m']\n",
    "        # m -> Margin\n",
    "        # P -> class-pairs in the current batch\n",
    "        # |P| -> Cardinality of P, number of elements in the set\n",
    "        \n",
    "        # print(mean_tensor.numpy())\n",
    "\n",
    "        dimension_tensor = K.shape(fx_tensor)[1]\n",
    "        num_classes_tensor = K.shape(y_true)[1]\n",
    "        num_samples_tensor = K.shape(y_true)[0]\n",
    "\n",
    "        # First we have to expand the mean matrix in order to substract it\n",
    "        mean_expanded = tf.broadcast_to(tf.expand_dims(mean_tensor, axis = 0), [num_classes_tensor, num_classes_tensor, dimension_tensor])\n",
    "        # We transpose it to do the subtraction\n",
    "        mean_trans = tf.transpose(mean_expanded, [1,0,2])\n",
    "\n",
    "        # We subtract it and do the norm through the \"dimensions\" axis.\n",
    "        norm = tf.subtract(mean_expanded, mean_trans)\n",
    "        norm = tf.reduce_sum(tf.multiply(norm, norm), axis = 2) \n",
    "\n",
    "        # Finally we subtract to m the previously calculated norm vector\n",
    "        m_tensor = tf.scalar_mul(m, tf.ones(K.shape(norm), tf.float32))\n",
    "        inside = tf.subtract(m_tensor, tf.cast(norm, tf.float32))\n",
    "\n",
    "        # Now we only take the lower part diagonal matrix, and as we cannot delete the diagonal, we subtract it in the final\n",
    "\n",
    "        mat = tf.linalg.LinearOperatorLowerTriangular(inside)\n",
    "        diagonal = mat.diag_part() # diagonal\n",
    "        mat = mat.to_dense() # Lower triangular matrix with diagonal\n",
    "\n",
    "        zeros = tf.zeros(K.shape(norm), tf.float32)\n",
    "        maximum = tf.maximum(mat, zeros)\n",
    "        max_squared = tf.square(maximum)\n",
    "\n",
    "        summ = tf.reduce_sum(max_squared)\n",
    "\n",
    "        diag_sum = tf.reduce_sum(tf.multiply(diagonal, diagonal))\n",
    "\n",
    "        summ = tf.subtract(summ, diag_sum)\n",
    "\n",
    "\n",
    "\n",
    "        counter = tf.ones([num_classes_tensor, num_classes_tensor], tf.float32)\n",
    "        counter = tf.linalg.LinearOperatorLowerTriangular(counter)\n",
    "\n",
    "        count_diag = counter.diag_part()\n",
    "        count_triang = counter.to_dense()\n",
    "        counter = tf.subtract(tf.reduce_sum(count_triang), tf.reduce_sum(count_diag))\n",
    "\n",
    "\n",
    "        loss = tf.multiply(tf.divide(gamma, tf.multiply(tf.constant(4, tf.float32), counter)), summ)\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    def model_loss(self):\n",
    "        \"\"\"\" Wrapper function which calculates auxiliary values for the complete loss function.\n",
    "         Returns a *function* which calculates the complete loss given only the input and target output \"\"\"\n",
    "        # This part has to be developed\n",
    "        \n",
    "        #Within class loss\n",
    "        within_class_loss_func = self.calculate_within_class_loss\n",
    "        \n",
    "        # Between class loss\n",
    "        between_class_loss_func = self.calculate_between_class_loss\n",
    "        \n",
    "        lay_out = self.model.layers[-2].output\n",
    "        \n",
    "        \n",
    "        def ElopeLoss(y_true, y_pred):\n",
    "            \n",
    "            # Within Class loss has to be computed first, in order to get the new class means updated\n",
    "            \n",
    "            within_class_loss = within_class_loss_func(y_true, y_pred, lay_out)\n",
    "            \n",
    "            between_class_loss = between_class_loss_func(y_true, y_pred, lay_out)\n",
    "            \n",
    "            cat_cross_loss = categorical_crossentropy(y_true, y_pred)\n",
    "            \n",
    "            model_loss = cat_cross_loss + within_class_loss + between_class_loss\n",
    "            \n",
    "            return model_loss\n",
    "        \n",
    "        \n",
    "        return ElopeLoss\n",
    "    \n",
    "    \n",
    "    def compile(self):\n",
    "        \"\"\" Compiles the Keras model. Includes metrics to differentiate between the two main loss terms \"\"\"\n",
    "        self.model.compile(optimizer = self.optimizer, loss = self.loss_func,\n",
    "                          metrics = [categorical_crossentropy]) # Here put the two losses better\n",
    "        print('Model Compiled!')\n",
    "        \n",
    "    \n",
    "    def load_trained_weights(self, weights):\n",
    "        \"\"\" Loads weights of a pre-trained model. 'weights' is path to h5 model\\weights file\"\"\"\n",
    "        self.model.load_weights(weights)\n",
    "        print('Weights from {} loaded successfully'.format(weights))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Elope = ElopeModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = Elope.model.fit(x_train[:120], y_train[:120], batch_size = 12, epochs = 1,\n",
    "                    validation_data = (x_val[:2], y_val[:2]), callbacks = [lr_schedule])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_file = open('testing.json', 'r')\n",
    "loaded_model_json = json_file.read()\n",
    "json_file.close()\n",
    "loaded_model = model_from_json(loaded_model_json, custom_objects = {'GlobalKMaxPooling2D': GlobalKMaxPooling2D})\n",
    "# load weights into new model\n",
    "loaded_model.load_weights(\"testing.h5\")\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory = 'C:/Users/julen/OneDrive/Escritorio/IA/CS577-Deep-Learning/Project/'\n",
    "df_test = pd.read_csv(directory + 'test.csv')\n",
    "x_test = load_images(df_test, directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_and_save(loaded_model, x_test, name = 'submit_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Verification of Between Class loss and Within Class loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to specify some random values and convert them to tensors\n",
    "\n",
    "f_x = np.random.rand(3,2048)\n",
    "mean = np.random.rand(4,2048)\n",
    "y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0]])\n",
    "\n",
    "fx_tensor = tf.convert_to_tensor(f_x)\n",
    "mean_tensor = tf.convert_to_tensor(mean)\n",
    "y_true_tensor = tf.convert_to_tensor(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fx_tensor.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now we are going to get the necessary slices from the tensors\n",
    "\n",
    "fx_tensor_slice = tf.slice(f_x, [1,0], [1, tf.shape(fx_tensor)[1].numpy()]) #Begin, size\n",
    "y_true_slice = tf.slice(y_true, [1,0], [1,4])\n",
    "class_number = np.argmax(y_true_slice.numpy()) # Class goes from 0 to 3, in order to fit better into slice method\n",
    "mean_slice = tf.slice(mean_tensor, [class_number, 0], [1,tf.shape(mean_tensor)[1].numpy()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('f_x shape: ', tf.shape(fx_tensor_slice))\n",
    "print('mean shape: ', tf.shape(mean_slice) )\n",
    "print('y_true shape: ', tf.shape(y_true_slice))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_slice - fx_tensor_slice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "K.eval(fx_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate loss with graph mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Within class loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 0.5\n",
    "num_classes = mean_tensor.shape[0]\n",
    "num_samples_in_batch = fx_tensor.shape[0]\n",
    "dimensions = mean_tensor.shape[1]\n",
    "\n",
    "# print('mean_tensor shape: ', mean_tensor.shape)\n",
    "# print('fx_tensor shape: ', fx_tensor.shape)\n",
    "# print('y_true shape: ', y_true.shape)\n",
    "\n",
    "dimension_tensor = K.shape(fx_tensor)[1]\n",
    "num_classes_tensor = K.shape(y_true)[1]\n",
    "num_samples_tensor = K.shape(y_true)[0]\n",
    "\n",
    "fx_expanded = tf.broadcast_to(tf.expand_dims(fx_tensor, axis = -1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "y_true_expanded = tf.broadcast_to(tf.expand_dims(y_true_tensor, axis = 1), [num_samples_tensor, dimension_tensor, num_classes_tensor])\n",
    "mean_expanded = tf.broadcast_to(tf.expand_dims(mean_tensor, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "\n",
    "mean_expanded = tf.transpose(mean_expanded, perm = [0,2,1])\n",
    "\n",
    "up = tf.reduce_sum(tf.multiply(tf.subtract(mean_expanded, fx_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)), axis = 0)\n",
    "\n",
    "y_true_cut = tf.reduce_sum(y_true_tensor, axis = 0)\n",
    "down = tf.add(y_true_cut, tf.constant(1))\n",
    "down = tf.cast(down, dtype = tf.dtypes.float64)\n",
    "delta = tf.divide(up, down)\n",
    "\n",
    "\n",
    "delta = tf.transpose(delta)\n",
    "\n",
    "mean_new = tf.subtract(mean_tensor, tf.scalar_mul(tf.constant(alpha, dtype = tf.dtypes.float64), delta))\n",
    "\n",
    "mean_new_sum = np.sum(mean_new.numpy())\n",
    "print()\n",
    "print(mean_new_sum)\n",
    "print(mean_new.numpy())\n",
    "print()\n",
    "print('---------------- NOW THE LOSS ---------------')\n",
    "print()\n",
    "\n",
    "# Now calculate the loss\n",
    "mean_new_expanded = tf.broadcast_to(tf.expand_dims(mean_new, axis = 0), [num_samples_tensor, num_classes_tensor, dimension_tensor])\n",
    "mean_new_expanded = tf.transpose(mean_new_expanded,  perm = [0,2,1]) \n",
    "\n",
    "print()\n",
    "inside = tf.reduce_sum(tf.multiply(tf.subtract(fx_expanded, mean_new_expanded), tf.cast(y_true_expanded, dtype = tf.dtypes.float64)), axis = 2)\n",
    "\n",
    "\n",
    "tot = tf.reduce_sum(tf.multiply(inside, inside)) # Apply the norm\n",
    "down = tf.multiply(tf.constant(2), num_samples_tensor)\n",
    "tot = tf.divide(tot, tf.cast(down, dtype = tf.dtypes.float64))\n",
    "\n",
    "print(tot.numpy())\n",
    "\n",
    "# mean_tensor = mean_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Between class loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "gamma = tf.constant(16, tf.float64)\n",
    "# print(mean_tensor.numpy())\n",
    "\n",
    "dimension_tensor = K.shape(fx_tensor)[1]\n",
    "num_classes_tensor = K.shape(y_true)[1]\n",
    "num_samples_tensor = K.shape(y_true)[0]\n",
    "print()\n",
    "print('dimension:', dimension_tensor.numpy())\n",
    "print('num_classes:', num_classes_tensor.numpy())\n",
    "print('num_samples:', num_samples_tensor.numpy())\n",
    "print()\n",
    "\n",
    "# First we have to expand the mean matrix in order to substract it\n",
    "mean_expanded = tf.broadcast_to(tf.expand_dims(mean_tensor, axis = 0), [num_classes_tensor, num_classes_tensor, dimension_tensor])\n",
    "# We transpose it to do the subtraction\n",
    "mean_trans = tf.transpose(mean_expanded, [1,0,2])\n",
    "\n",
    "# We subtract it and do the norm through the \"dimensions\" axis.\n",
    "norm = tf.subtract(mean_expanded, mean_trans)\n",
    "norm = tf.reduce_sum(tf.multiply(norm, norm), axis = 2) \n",
    "\n",
    "# Finally we subtract to m the previously calculated norm vector\n",
    "m_tensor = tf.scalar_mul(tf.constant(m, tf.float64), tf.ones(norm.shape, tf.float64))\n",
    "inside = tf.subtract(m_tensor, norm)\n",
    "\n",
    "\n",
    "# print(inside.numpy())\n",
    "print()\n",
    "\n",
    "# Now we only take the lower part diagonal matrix, and as we cannot delete the diagonal, we subtract it in the final\n",
    "print()\n",
    "mat = tf.linalg.LinearOperatorLowerTriangular(inside)\n",
    "diagonal = mat.diag_part() # diagonal\n",
    "mat = mat.to_dense() # Lower triangular matrix with diagonal\n",
    "\n",
    "zeros = tf.zeros(norm.shape, tf.float64)\n",
    "maximum = tf.maximum(mat, zeros)\n",
    "max_squared = tf.square(maximum)\n",
    "# print(max_squared.numpy())\n",
    "summ = tf.reduce_sum(max_squared)\n",
    "# print('After ')\n",
    "# print(summ.numpy())\n",
    "diag_sum = tf.reduce_sum(tf.multiply(diagonal, diagonal))\n",
    "# print(diag_sum.numpy())\n",
    "summ = tf.subtract(summ, diag_sum)\n",
    "# print(summ.numpy())\n",
    "\n",
    "\n",
    "counter = tf.ones([num_classes_tensor, num_classes_tensor], tf.float64)\n",
    "counter = tf.linalg.LinearOperatorLowerTriangular(counter)\n",
    "\n",
    "count_diag = counter.diag_part()\n",
    "count_triang = counter.to_dense()\n",
    "counter = tf.subtract(tf.reduce_sum(count_triang), tf.reduce_sum(count_diag))\n",
    "\n",
    "\n",
    "loss = tf.multiply(tf.divide(gamma, tf.multiply(4, counter)), summ)\n",
    "\n",
    "\n",
    "# loss = (gamma/(4*counter)) *  summ.numpy()\n",
    "\n",
    "\n",
    "print(loss.numpy())\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement it in Numpy to see it is correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First we are going to specify some random values and convert them to tensors\n",
    "\n",
    "f_x = np.random.rand(6,2048) \n",
    "mean = np.random.rand(4,2048) \n",
    "y_true = np.array([[0,0,1,0], [1,0,0,0], [0,1,0,0], [0,0,0,1], [1,0,0,0], [0,1,0,0]])\n",
    "\n",
    "# f_x = np.array([[2,3,4,5], [-1, 0, -2, -3], [5,6, 8, 6]], dtype = 'float64')\n",
    "# mean = np.array([[-2, -1, -3, -2], [4, 5, 7, 6]], dtype = 'float64')\n",
    "# y_true = np.array([[0,1], [1,0], [0,1]])\n",
    "\n",
    "fx_tensor = tf.convert_to_tensor(f_x, dtype = tf.dtypes.float64)\n",
    "mean_tensor = tf.convert_to_tensor(mean, dtype = tf.dtypes.float64)\n",
    "y_true_tensor = tf.convert_to_tensor(y_true)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is well done!\n",
    "alpha = 0.5\n",
    "num_classes = 2\n",
    "num_samples_in_batch = 3\n",
    "dimension = 4\n",
    "\n",
    "# LOOP THROUGH CLASSES TO CALCULATE NEW MEAN\n",
    "for i in range(0, num_classes): # Loops throgh the classes\n",
    "\n",
    "    counter = 1 # to divide it\n",
    "    summ = np.zeros(dimension)  # Initialize class means \n",
    "    mean_i = mean[i]\n",
    "    for j in range(0, num_samples_in_batch): # Loops through the images in the batch  \n",
    "        y_true_sample = y_true[j] # Class of the i image\n",
    "        class_number = np.argmax(y_true_sample) \n",
    "        \n",
    "        if class_number == i: # if not, do nothing\n",
    "            counter += 1\n",
    "            summ = summ + (mean_i - f_x[j])\n",
    "        \n",
    "    total = summ / counter\n",
    "    mean[i] = mean_i - alpha*total \n",
    "    \n",
    "print('The new mean: ', mean)\n",
    "print()\n",
    "\n",
    "# Calculate the loss\n",
    "suma = 0\n",
    "for i in range(0, num_samples_in_batch):\n",
    "    y_true_sample = y_true[i] # Class of the i image\n",
    "    class_number = np.argmax(y_true_sample)\n",
    "    inside = f_x[i] - mean[class_number]\n",
    "    norm = np.linalg.norm(inside)\n",
    "    squared = np.square(norm)\n",
    "    suma = suma + np.square(np.linalg.norm(f_x[i] - mean[class_number]))\n",
    "loss = suma / (2*num_samples_in_batch)\n",
    "print('The final loss:', loss)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implement the Between Class loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 1000\n",
    "summ = 0 \n",
    "counter = 0\n",
    "gamma = 16\n",
    "# print(mean)\n",
    "print()\n",
    "for i in range(0, len(mean)):\n",
    "    for j in range(i, len(mean)):\n",
    "        if i != j:\n",
    "            summ = summ + np.square(max(m - np.square(np.linalg.norm(mean[i] - mean[j])), 0))\n",
    "#             print(mean[i])\n",
    "#             print(mean[j])\n",
    "#             print()\n",
    "            counter += 1\n",
    "loss = (gamma/(4*counter)) * summ\n",
    "print(counter)\n",
    "loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
